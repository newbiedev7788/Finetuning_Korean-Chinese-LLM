{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 17.48633879781421,
  "eval_steps": 500,
  "global_step": 800,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.11,
      "grad_norm": 0.4190762937068939,
      "learning_rate": 4.999939076763487e-05,
      "loss": 4.0813,
      "step": 5
    },
    {
      "epoch": 0.22,
      "grad_norm": 0.5682299137115479,
      "learning_rate": 4.999756310023261e-05,
      "loss": 3.8759,
      "step": 10
    },
    {
      "epoch": 0.33,
      "grad_norm": 0.6293856501579285,
      "learning_rate": 4.999451708687114e-05,
      "loss": 3.9084,
      "step": 15
    },
    {
      "epoch": 0.44,
      "grad_norm": 0.8758161664009094,
      "learning_rate": 4.999025287600886e-05,
      "loss": 3.8482,
      "step": 20
    },
    {
      "epoch": 0.55,
      "grad_norm": 1.1061590909957886,
      "learning_rate": 4.99847706754774e-05,
      "loss": 3.9143,
      "step": 25
    },
    {
      "epoch": 0.66,
      "grad_norm": 1.0389021635055542,
      "learning_rate": 4.997807075247146e-05,
      "loss": 3.8069,
      "step": 30
    },
    {
      "epoch": 0.77,
      "grad_norm": 1.7076208591461182,
      "learning_rate": 4.997015343353585e-05,
      "loss": 3.7051,
      "step": 35
    },
    {
      "epoch": 0.87,
      "grad_norm": 1.0512803792953491,
      "learning_rate": 4.996101910454953e-05,
      "loss": 3.7443,
      "step": 40
    },
    {
      "epoch": 0.98,
      "grad_norm": 0.8518654704093933,
      "learning_rate": 4.995066821070679e-05,
      "loss": 3.5774,
      "step": 45
    },
    {
      "epoch": 1.09,
      "grad_norm": 0.9860131144523621,
      "learning_rate": 4.993910125649561e-05,
      "loss": 3.5481,
      "step": 50
    },
    {
      "epoch": 1.2,
      "grad_norm": 1.0826561450958252,
      "learning_rate": 4.992631880567301e-05,
      "loss": 3.4416,
      "step": 55
    },
    {
      "epoch": 1.31,
      "grad_norm": 0.8481911420822144,
      "learning_rate": 4.991232148123761e-05,
      "loss": 3.4859,
      "step": 60
    },
    {
      "epoch": 1.42,
      "grad_norm": 1.614713191986084,
      "learning_rate": 4.989710996539926e-05,
      "loss": 3.479,
      "step": 65
    },
    {
      "epoch": 1.53,
      "grad_norm": 0.9479525685310364,
      "learning_rate": 4.988068499954578e-05,
      "loss": 3.3769,
      "step": 70
    },
    {
      "epoch": 1.64,
      "grad_norm": 0.9738621115684509,
      "learning_rate": 4.9863047384206835e-05,
      "loss": 3.3201,
      "step": 75
    },
    {
      "epoch": 1.75,
      "grad_norm": 0.7980673909187317,
      "learning_rate": 4.984419797901491e-05,
      "loss": 3.3725,
      "step": 80
    },
    {
      "epoch": 1.86,
      "grad_norm": 0.7634841203689575,
      "learning_rate": 4.982413770266342e-05,
      "loss": 3.3198,
      "step": 85
    },
    {
      "epoch": 1.97,
      "grad_norm": 1.6109321117401123,
      "learning_rate": 4.980286753286195e-05,
      "loss": 3.3547,
      "step": 90
    },
    {
      "epoch": 2.08,
      "grad_norm": 0.8682252168655396,
      "learning_rate": 4.978038850628854e-05,
      "loss": 3.2288,
      "step": 95
    },
    {
      "epoch": 2.19,
      "grad_norm": 0.9758785963058472,
      "learning_rate": 4.975670171853926e-05,
      "loss": 3.2202,
      "step": 100
    },
    {
      "epoch": 2.3,
      "grad_norm": 1.0699541568756104,
      "learning_rate": 4.9731808324074717e-05,
      "loss": 3.2788,
      "step": 105
    },
    {
      "epoch": 2.4,
      "grad_norm": 1.163353681564331,
      "learning_rate": 4.9705709536163824e-05,
      "loss": 3.2881,
      "step": 110
    },
    {
      "epoch": 2.51,
      "grad_norm": 1.2735884189605713,
      "learning_rate": 4.96784066268247e-05,
      "loss": 3.2715,
      "step": 115
    },
    {
      "epoch": 2.62,
      "grad_norm": 0.9545009732246399,
      "learning_rate": 4.964990092676263e-05,
      "loss": 3.2181,
      "step": 120
    },
    {
      "epoch": 2.73,
      "grad_norm": 1.3793822526931763,
      "learning_rate": 4.962019382530521e-05,
      "loss": 3.1458,
      "step": 125
    },
    {
      "epoch": 2.84,
      "grad_norm": 1.2555819749832153,
      "learning_rate": 4.9589286770334654e-05,
      "loss": 3.247,
      "step": 130
    },
    {
      "epoch": 2.95,
      "grad_norm": 1.3082062005996704,
      "learning_rate": 4.9557181268217227e-05,
      "loss": 3.0994,
      "step": 135
    },
    {
      "epoch": 3.06,
      "grad_norm": 1.440890908241272,
      "learning_rate": 4.952387888372979e-05,
      "loss": 3.065,
      "step": 140
    },
    {
      "epoch": 3.17,
      "grad_norm": 1.061680555343628,
      "learning_rate": 4.94893812399836e-05,
      "loss": 3.1024,
      "step": 145
    },
    {
      "epoch": 3.28,
      "grad_norm": 1.0604188442230225,
      "learning_rate": 4.9453690018345144e-05,
      "loss": 3.1348,
      "step": 150
    },
    {
      "epoch": 3.39,
      "grad_norm": 0.9158951640129089,
      "learning_rate": 4.94168069583542e-05,
      "loss": 3.0929,
      "step": 155
    },
    {
      "epoch": 3.5,
      "grad_norm": 1.9503084421157837,
      "learning_rate": 4.937873385763908e-05,
      "loss": 3.1471,
      "step": 160
    },
    {
      "epoch": 3.61,
      "grad_norm": 1.6482278108596802,
      "learning_rate": 4.933947257182901e-05,
      "loss": 3.1082,
      "step": 165
    },
    {
      "epoch": 3.72,
      "grad_norm": 1.8090347051620483,
      "learning_rate": 4.929902501446366e-05,
      "loss": 3.0517,
      "step": 170
    },
    {
      "epoch": 3.83,
      "grad_norm": 1.215492844581604,
      "learning_rate": 4.925739315689991e-05,
      "loss": 3.0187,
      "step": 175
    },
    {
      "epoch": 3.93,
      "grad_norm": 1.305962085723877,
      "learning_rate": 4.9214579028215776e-05,
      "loss": 3.0567,
      "step": 180
    },
    {
      "epoch": 4.04,
      "grad_norm": 1.4741570949554443,
      "learning_rate": 4.917058471511149e-05,
      "loss": 3.1851,
      "step": 185
    },
    {
      "epoch": 4.15,
      "grad_norm": 1.134225606918335,
      "learning_rate": 4.912541236180779e-05,
      "loss": 3.1119,
      "step": 190
    },
    {
      "epoch": 4.26,
      "grad_norm": 1.534368634223938,
      "learning_rate": 4.907906416994146e-05,
      "loss": 2.9748,
      "step": 195
    },
    {
      "epoch": 4.37,
      "grad_norm": 1.434862732887268,
      "learning_rate": 4.9031542398457974e-05,
      "loss": 3.0311,
      "step": 200
    },
    {
      "epoch": 4.48,
      "grad_norm": 1.204943299293518,
      "learning_rate": 4.898284936350144e-05,
      "loss": 2.9843,
      "step": 205
    },
    {
      "epoch": 4.59,
      "grad_norm": 1.3439704179763794,
      "learning_rate": 4.893298743830168e-05,
      "loss": 3.0541,
      "step": 210
    },
    {
      "epoch": 4.7,
      "grad_norm": 2.6146345138549805,
      "learning_rate": 4.888195905305859e-05,
      "loss": 2.9563,
      "step": 215
    },
    {
      "epoch": 4.81,
      "grad_norm": 1.5610507726669312,
      "learning_rate": 4.882976669482367e-05,
      "loss": 2.9508,
      "step": 220
    },
    {
      "epoch": 4.92,
      "grad_norm": 4.328429698944092,
      "learning_rate": 4.877641290737884e-05,
      "loss": 2.9442,
      "step": 225
    },
    {
      "epoch": 5.03,
      "grad_norm": 1.6496464014053345,
      "learning_rate": 4.8721900291112415e-05,
      "loss": 2.9096,
      "step": 230
    },
    {
      "epoch": 5.14,
      "grad_norm": 1.2491257190704346,
      "learning_rate": 4.8666231502892415e-05,
      "loss": 2.9487,
      "step": 235
    },
    {
      "epoch": 5.25,
      "grad_norm": 2.3536081314086914,
      "learning_rate": 4.860940925593703e-05,
      "loss": 3.0078,
      "step": 240
    },
    {
      "epoch": 5.36,
      "grad_norm": 1.104355812072754,
      "learning_rate": 4.855143631968242e-05,
      "loss": 2.9357,
      "step": 245
    },
    {
      "epoch": 5.46,
      "grad_norm": 1.5648047924041748,
      "learning_rate": 4.849231551964771e-05,
      "loss": 2.9396,
      "step": 250
    },
    {
      "epoch": 5.57,
      "grad_norm": 1.7331169843673706,
      "learning_rate": 4.843204973729729e-05,
      "loss": 2.8022,
      "step": 255
    },
    {
      "epoch": 5.68,
      "grad_norm": 1.7644976377487183,
      "learning_rate": 4.837064190990036e-05,
      "loss": 2.9409,
      "step": 260
    },
    {
      "epoch": 5.79,
      "grad_norm": 1.7957509756088257,
      "learning_rate": 4.830809503038781e-05,
      "loss": 2.9713,
      "step": 265
    },
    {
      "epoch": 5.9,
      "grad_norm": 1.6274524927139282,
      "learning_rate": 4.8244412147206284e-05,
      "loss": 2.8444,
      "step": 270
    },
    {
      "epoch": 6.01,
      "grad_norm": 2.742912769317627,
      "learning_rate": 4.817959636416969e-05,
      "loss": 2.8739,
      "step": 275
    },
    {
      "epoch": 6.12,
      "grad_norm": 1.5542943477630615,
      "learning_rate": 4.8113650840307834e-05,
      "loss": 2.7814,
      "step": 280
    },
    {
      "epoch": 6.23,
      "grad_norm": 1.5975605249404907,
      "learning_rate": 4.8046578789712515e-05,
      "loss": 2.8844,
      "step": 285
    },
    {
      "epoch": 6.34,
      "grad_norm": 2.1223111152648926,
      "learning_rate": 4.797838348138086e-05,
      "loss": 2.9603,
      "step": 290
    },
    {
      "epoch": 6.45,
      "grad_norm": 3.0871102809906006,
      "learning_rate": 4.790906823905599e-05,
      "loss": 2.8727,
      "step": 295
    },
    {
      "epoch": 6.56,
      "grad_norm": 2.3582935333251953,
      "learning_rate": 4.783863644106502e-05,
      "loss": 2.7959,
      "step": 300
    },
    {
      "epoch": 6.67,
      "grad_norm": 1.5558550357818604,
      "learning_rate": 4.776709152015443e-05,
      "loss": 2.9485,
      "step": 305
    },
    {
      "epoch": 6.78,
      "grad_norm": 1.6382948160171509,
      "learning_rate": 4.769443696332272e-05,
      "loss": 2.8093,
      "step": 310
    },
    {
      "epoch": 6.89,
      "grad_norm": 2.0600132942199707,
      "learning_rate": 4.762067631165049e-05,
      "loss": 2.8527,
      "step": 315
    },
    {
      "epoch": 6.99,
      "grad_norm": 1.3371963500976562,
      "learning_rate": 4.754581316012785e-05,
      "loss": 2.887,
      "step": 320
    },
    {
      "epoch": 7.1,
      "grad_norm": 1.5286413431167603,
      "learning_rate": 4.7469851157479177e-05,
      "loss": 2.9055,
      "step": 325
    },
    {
      "epoch": 7.21,
      "grad_norm": 2.8490235805511475,
      "learning_rate": 4.7392794005985326e-05,
      "loss": 2.9468,
      "step": 330
    },
    {
      "epoch": 7.32,
      "grad_norm": 1.9624965190887451,
      "learning_rate": 4.731464546130314e-05,
      "loss": 2.7907,
      "step": 335
    },
    {
      "epoch": 7.43,
      "grad_norm": 2.847702741622925,
      "learning_rate": 4.723540933228244e-05,
      "loss": 2.8365,
      "step": 340
    },
    {
      "epoch": 7.54,
      "grad_norm": 1.4034184217453003,
      "learning_rate": 4.715508948078037e-05,
      "loss": 2.8511,
      "step": 345
    },
    {
      "epoch": 7.65,
      "grad_norm": 1.788514256477356,
      "learning_rate": 4.707368982147318e-05,
      "loss": 2.857,
      "step": 350
    },
    {
      "epoch": 7.76,
      "grad_norm": 1.9963618516921997,
      "learning_rate": 4.6991214321665414e-05,
      "loss": 2.8112,
      "step": 355
    },
    {
      "epoch": 7.87,
      "grad_norm": 1.9567127227783203,
      "learning_rate": 4.690766700109659e-05,
      "loss": 2.8015,
      "step": 360
    },
    {
      "epoch": 7.98,
      "grad_norm": 1.9742763042449951,
      "learning_rate": 4.682305193174524e-05,
      "loss": 2.8676,
      "step": 365
    },
    {
      "epoch": 8.09,
      "grad_norm": 2.649463653564453,
      "learning_rate": 4.6737373237630476e-05,
      "loss": 2.8847,
      "step": 370
    },
    {
      "epoch": 8.2,
      "grad_norm": 1.6693305969238281,
      "learning_rate": 4.665063509461097e-05,
      "loss": 2.793,
      "step": 375
    },
    {
      "epoch": 8.31,
      "grad_norm": 2.089420795440674,
      "learning_rate": 4.656284173018144e-05,
      "loss": 2.8681,
      "step": 380
    },
    {
      "epoch": 8.42,
      "grad_norm": 1.9591645002365112,
      "learning_rate": 4.6473997423266614e-05,
      "loss": 2.7823,
      "step": 385
    },
    {
      "epoch": 8.52,
      "grad_norm": 1.6575186252593994,
      "learning_rate": 4.638410650401267e-05,
      "loss": 2.7369,
      "step": 390
    },
    {
      "epoch": 8.63,
      "grad_norm": 1.923897385597229,
      "learning_rate": 4.629317335357619e-05,
      "loss": 2.7591,
      "step": 395
    },
    {
      "epoch": 8.74,
      "grad_norm": 1.9056206941604614,
      "learning_rate": 4.620120240391065e-05,
      "loss": 2.8194,
      "step": 400
    },
    {
      "epoch": 8.85,
      "grad_norm": 2.3751018047332764,
      "learning_rate": 4.610819813755038e-05,
      "loss": 2.7967,
      "step": 405
    },
    {
      "epoch": 8.96,
      "grad_norm": 1.841272234916687,
      "learning_rate": 4.601416508739211e-05,
      "loss": 2.7428,
      "step": 410
    },
    {
      "epoch": 9.07,
      "grad_norm": 2.284749746322632,
      "learning_rate": 4.591910783647404e-05,
      "loss": 2.7538,
      "step": 415
    },
    {
      "epoch": 9.18,
      "grad_norm": 3.3288071155548096,
      "learning_rate": 4.5823031017752485e-05,
      "loss": 2.7747,
      "step": 420
    },
    {
      "epoch": 9.29,
      "grad_norm": 1.6092368364334106,
      "learning_rate": 4.572593931387604e-05,
      "loss": 2.7137,
      "step": 425
    },
    {
      "epoch": 9.4,
      "grad_norm": 1.6199012994766235,
      "learning_rate": 4.562783745695738e-05,
      "loss": 2.7412,
      "step": 430
    },
    {
      "epoch": 9.51,
      "grad_norm": 2.245915174484253,
      "learning_rate": 4.5528730228342605e-05,
      "loss": 2.8198,
      "step": 435
    },
    {
      "epoch": 9.62,
      "grad_norm": 2.8612046241760254,
      "learning_rate": 4.542862245837821e-05,
      "loss": 2.7521,
      "step": 440
    },
    {
      "epoch": 9.73,
      "grad_norm": 1.9747025966644287,
      "learning_rate": 4.532751902617569e-05,
      "loss": 2.8232,
      "step": 445
    },
    {
      "epoch": 9.84,
      "grad_norm": 1.9523286819458008,
      "learning_rate": 4.522542485937369e-05,
      "loss": 2.7549,
      "step": 450
    },
    {
      "epoch": 9.95,
      "grad_norm": 2.9748637676239014,
      "learning_rate": 4.512234493389785e-05,
      "loss": 2.9136,
      "step": 455
    },
    {
      "epoch": 10.05,
      "grad_norm": 2.143521308898926,
      "learning_rate": 4.5018284273718336e-05,
      "loss": 2.7354,
      "step": 460
    },
    {
      "epoch": 10.16,
      "grad_norm": 1.9173630475997925,
      "learning_rate": 4.491324795060491e-05,
      "loss": 2.8792,
      "step": 465
    },
    {
      "epoch": 10.27,
      "grad_norm": 1.591972827911377,
      "learning_rate": 4.480724108387977e-05,
      "loss": 2.8607,
      "step": 470
    },
    {
      "epoch": 10.38,
      "grad_norm": 2.376676559448242,
      "learning_rate": 4.4700268840168045e-05,
      "loss": 2.6714,
      "step": 475
    },
    {
      "epoch": 10.49,
      "grad_norm": 1.9637905359268188,
      "learning_rate": 4.4592336433146e-05,
      "loss": 2.6977,
      "step": 480
    },
    {
      "epoch": 10.6,
      "grad_norm": 1.783913016319275,
      "learning_rate": 4.448344912328686e-05,
      "loss": 2.8091,
      "step": 485
    },
    {
      "epoch": 10.71,
      "grad_norm": 2.785374879837036,
      "learning_rate": 4.4373612217604496e-05,
      "loss": 2.6703,
      "step": 490
    },
    {
      "epoch": 10.82,
      "grad_norm": 2.0408742427825928,
      "learning_rate": 4.426283106939474e-05,
      "loss": 2.7865,
      "step": 495
    },
    {
      "epoch": 10.93,
      "grad_norm": 1.5654276609420776,
      "learning_rate": 4.415111107797445e-05,
      "loss": 2.6731,
      "step": 500
    },
    {
      "epoch": 11.04,
      "grad_norm": 2.173470973968506,
      "learning_rate": 4.403845768841842e-05,
      "loss": 2.6599,
      "step": 505
    },
    {
      "epoch": 11.15,
      "grad_norm": 3.213702440261841,
      "learning_rate": 4.3924876391293915e-05,
      "loss": 2.7658,
      "step": 510
    },
    {
      "epoch": 11.26,
      "grad_norm": 1.9662039279937744,
      "learning_rate": 4.381037272239311e-05,
      "loss": 2.6776,
      "step": 515
    },
    {
      "epoch": 11.37,
      "grad_norm": 1.6718438863754272,
      "learning_rate": 4.36949522624633e-05,
      "loss": 2.6428,
      "step": 520
    },
    {
      "epoch": 11.48,
      "grad_norm": 2.3357784748077393,
      "learning_rate": 4.357862063693486e-05,
      "loss": 2.7695,
      "step": 525
    },
    {
      "epoch": 11.58,
      "grad_norm": 2.11918306350708,
      "learning_rate": 4.3461383515647106e-05,
      "loss": 2.6464,
      "step": 530
    },
    {
      "epoch": 11.69,
      "grad_norm": 3.0614821910858154,
      "learning_rate": 4.334324661257191e-05,
      "loss": 2.7032,
      "step": 535
    },
    {
      "epoch": 11.8,
      "grad_norm": 1.9712938070297241,
      "learning_rate": 4.3224215685535294e-05,
      "loss": 2.6588,
      "step": 540
    },
    {
      "epoch": 11.91,
      "grad_norm": 2.469219207763672,
      "learning_rate": 4.3104296535936695e-05,
      "loss": 2.6809,
      "step": 545
    },
    {
      "epoch": 12.02,
      "grad_norm": 1.6632431745529175,
      "learning_rate": 4.2983495008466276e-05,
      "loss": 2.8413,
      "step": 550
    },
    {
      "epoch": 12.13,
      "grad_norm": 2.7971320152282715,
      "learning_rate": 4.2861816990820084e-05,
      "loss": 2.7944,
      "step": 555
    },
    {
      "epoch": 12.24,
      "grad_norm": 1.8295047283172607,
      "learning_rate": 4.273926841341302e-05,
      "loss": 2.6972,
      "step": 560
    },
    {
      "epoch": 12.35,
      "grad_norm": 1.9537142515182495,
      "learning_rate": 4.261585524908987e-05,
      "loss": 2.6321,
      "step": 565
    },
    {
      "epoch": 12.46,
      "grad_norm": 1.6536171436309814,
      "learning_rate": 4.249158351283414e-05,
      "loss": 2.5501,
      "step": 570
    },
    {
      "epoch": 12.57,
      "grad_norm": 2.082364320755005,
      "learning_rate": 4.2366459261474933e-05,
      "loss": 2.7045,
      "step": 575
    },
    {
      "epoch": 12.68,
      "grad_norm": 3.1343138217926025,
      "learning_rate": 4.224048859339175e-05,
      "loss": 2.6364,
      "step": 580
    },
    {
      "epoch": 12.79,
      "grad_norm": 2.99122953414917,
      "learning_rate": 4.211367764821722e-05,
      "loss": 2.6503,
      "step": 585
    },
    {
      "epoch": 12.9,
      "grad_norm": 2.024829864501953,
      "learning_rate": 4.198603260653792e-05,
      "loss": 2.6453,
      "step": 590
    },
    {
      "epoch": 13.01,
      "grad_norm": 2.1013944149017334,
      "learning_rate": 4.185755968959308e-05,
      "loss": 2.6756,
      "step": 595
    },
    {
      "epoch": 13.11,
      "grad_norm": 3.345149517059326,
      "learning_rate": 4.172826515897146e-05,
      "loss": 2.6072,
      "step": 600
    },
    {
      "epoch": 13.22,
      "grad_norm": 2.0921173095703125,
      "learning_rate": 4.1598155316306044e-05,
      "loss": 2.6775,
      "step": 605
    },
    {
      "epoch": 13.33,
      "grad_norm": 1.9780112504959106,
      "learning_rate": 4.146723650296701e-05,
      "loss": 2.6798,
      "step": 610
    },
    {
      "epoch": 13.44,
      "grad_norm": 2.3585338592529297,
      "learning_rate": 4.133551509975264e-05,
      "loss": 2.5418,
      "step": 615
    },
    {
      "epoch": 13.55,
      "grad_norm": 2.106301784515381,
      "learning_rate": 4.1202997526578276e-05,
      "loss": 2.6178,
      "step": 620
    },
    {
      "epoch": 13.66,
      "grad_norm": 2.444201707839966,
      "learning_rate": 4.1069690242163484e-05,
      "loss": 2.7604,
      "step": 625
    },
    {
      "epoch": 13.77,
      "grad_norm": 1.8051451444625854,
      "learning_rate": 4.093559974371725e-05,
      "loss": 2.7294,
      "step": 630
    },
    {
      "epoch": 13.88,
      "grad_norm": 2.9431896209716797,
      "learning_rate": 4.080073256662127e-05,
      "loss": 2.6389,
      "step": 635
    },
    {
      "epoch": 13.99,
      "grad_norm": 3.060418128967285,
      "learning_rate": 4.066509528411152e-05,
      "loss": 2.6634,
      "step": 640
    },
    {
      "epoch": 14.1,
      "grad_norm": 2.0075290203094482,
      "learning_rate": 4.052869450695776e-05,
      "loss": 2.6916,
      "step": 645
    },
    {
      "epoch": 14.21,
      "grad_norm": 2.7386865615844727,
      "learning_rate": 4.039153688314145e-05,
      "loss": 2.706,
      "step": 650
    },
    {
      "epoch": 14.32,
      "grad_norm": 2.882669687271118,
      "learning_rate": 4.02536290975317e-05,
      "loss": 2.5839,
      "step": 655
    },
    {
      "epoch": 14.43,
      "grad_norm": 2.7063851356506348,
      "learning_rate": 4.011497787155938e-05,
      "loss": 2.6345,
      "step": 660
    },
    {
      "epoch": 14.54,
      "grad_norm": 1.7728866338729858,
      "learning_rate": 3.997558996288965e-05,
      "loss": 2.6113,
      "step": 665
    },
    {
      "epoch": 14.64,
      "grad_norm": 2.0899062156677246,
      "learning_rate": 3.983547216509254e-05,
      "loss": 2.6386,
      "step": 670
    },
    {
      "epoch": 14.75,
      "grad_norm": 2.966677665710449,
      "learning_rate": 3.969463130731183e-05,
      "loss": 2.6404,
      "step": 675
    },
    {
      "epoch": 14.86,
      "grad_norm": 1.8099416494369507,
      "learning_rate": 3.955307425393224e-05,
      "loss": 2.6149,
      "step": 680
    },
    {
      "epoch": 14.97,
      "grad_norm": 1.9740931987762451,
      "learning_rate": 3.941080790424484e-05,
      "loss": 2.5981,
      "step": 685
    },
    {
      "epoch": 15.08,
      "grad_norm": 2.01714825630188,
      "learning_rate": 3.92678391921108e-05,
      "loss": 2.564,
      "step": 690
    },
    {
      "epoch": 15.19,
      "grad_norm": 2.4252054691314697,
      "learning_rate": 3.912417508562345e-05,
      "loss": 2.5909,
      "step": 695
    },
    {
      "epoch": 15.3,
      "grad_norm": 2.0233445167541504,
      "learning_rate": 3.897982258676867e-05,
      "loss": 2.5756,
      "step": 700
    },
    {
      "epoch": 15.41,
      "grad_norm": 1.9049184322357178,
      "learning_rate": 3.883478873108361e-05,
      "loss": 2.6182,
      "step": 705
    },
    {
      "epoch": 15.52,
      "grad_norm": 2.807706832885742,
      "learning_rate": 3.868908058731376e-05,
      "loss": 2.6562,
      "step": 710
    },
    {
      "epoch": 15.63,
      "grad_norm": 3.0991296768188477,
      "learning_rate": 3.85427052570685e-05,
      "loss": 2.6589,
      "step": 715
    },
    {
      "epoch": 15.74,
      "grad_norm": 3.1019067764282227,
      "learning_rate": 3.8395669874474915e-05,
      "loss": 2.5162,
      "step": 720
    },
    {
      "epoch": 15.85,
      "grad_norm": 1.8713304996490479,
      "learning_rate": 3.824798160583012e-05,
      "loss": 2.522,
      "step": 725
    },
    {
      "epoch": 15.96,
      "grad_norm": 2.5536465644836426,
      "learning_rate": 3.8099647649251986e-05,
      "loss": 2.6374,
      "step": 730
    },
    {
      "epoch": 16.07,
      "grad_norm": 2.2478325366973877,
      "learning_rate": 3.795067523432826e-05,
      "loss": 2.755,
      "step": 735
    },
    {
      "epoch": 16.17,
      "grad_norm": 1.648483395576477,
      "learning_rate": 3.780107162176429e-05,
      "loss": 2.568,
      "step": 740
    },
    {
      "epoch": 16.28,
      "grad_norm": 3.7869274616241455,
      "learning_rate": 3.765084410302909e-05,
      "loss": 2.555,
      "step": 745
    },
    {
      "epoch": 16.39,
      "grad_norm": 3.670029401779175,
      "learning_rate": 3.7500000000000003e-05,
      "loss": 2.6452,
      "step": 750
    },
    {
      "epoch": 16.5,
      "grad_norm": 2.783968925476074,
      "learning_rate": 3.7348546664605777e-05,
      "loss": 2.5808,
      "step": 755
    },
    {
      "epoch": 16.61,
      "grad_norm": 2.4255223274230957,
      "learning_rate": 3.719649147846832e-05,
      "loss": 2.5399,
      "step": 760
    },
    {
      "epoch": 16.72,
      "grad_norm": 3.265207052230835,
      "learning_rate": 3.704384185254288e-05,
      "loss": 2.6018,
      "step": 765
    },
    {
      "epoch": 16.83,
      "grad_norm": 2.012962818145752,
      "learning_rate": 3.689060522675689e-05,
      "loss": 2.5453,
      "step": 770
    },
    {
      "epoch": 16.94,
      "grad_norm": 2.975487232208252,
      "learning_rate": 3.673678906964727e-05,
      "loss": 2.6118,
      "step": 775
    },
    {
      "epoch": 17.05,
      "grad_norm": 2.437328338623047,
      "learning_rate": 3.6582400877996546e-05,
      "loss": 2.5509,
      "step": 780
    },
    {
      "epoch": 17.16,
      "grad_norm": 3.113149404525757,
      "learning_rate": 3.642744817646736e-05,
      "loss": 2.638,
      "step": 785
    },
    {
      "epoch": 17.27,
      "grad_norm": 3.129086971282959,
      "learning_rate": 3.627193851723577e-05,
      "loss": 2.5605,
      "step": 790
    },
    {
      "epoch": 17.38,
      "grad_norm": 3.720142126083374,
      "learning_rate": 3.611587947962319e-05,
      "loss": 2.558,
      "step": 795
    },
    {
      "epoch": 17.49,
      "grad_norm": 2.1802942752838135,
      "learning_rate": 3.5959278669726935e-05,
      "loss": 2.4937,
      "step": 800
    }
  ],
  "logging_steps": 5,
  "max_steps": 2250,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 50,
  "save_steps": 100,
  "total_flos": 3.6774462472382054e+17,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
