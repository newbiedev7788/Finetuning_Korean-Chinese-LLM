{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 49.18032786885246,
  "eval_steps": 500,
  "global_step": 2250,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.11,
      "grad_norm": 0.4190762937068939,
      "learning_rate": 4.999939076763487e-05,
      "loss": 4.0813,
      "step": 5
    },
    {
      "epoch": 0.22,
      "grad_norm": 0.5682299137115479,
      "learning_rate": 4.999756310023261e-05,
      "loss": 3.8759,
      "step": 10
    },
    {
      "epoch": 0.33,
      "grad_norm": 0.6293856501579285,
      "learning_rate": 4.999451708687114e-05,
      "loss": 3.9084,
      "step": 15
    },
    {
      "epoch": 0.44,
      "grad_norm": 0.8758161664009094,
      "learning_rate": 4.999025287600886e-05,
      "loss": 3.8482,
      "step": 20
    },
    {
      "epoch": 0.55,
      "grad_norm": 1.1061590909957886,
      "learning_rate": 4.99847706754774e-05,
      "loss": 3.9143,
      "step": 25
    },
    {
      "epoch": 0.66,
      "grad_norm": 1.0389021635055542,
      "learning_rate": 4.997807075247146e-05,
      "loss": 3.8069,
      "step": 30
    },
    {
      "epoch": 0.77,
      "grad_norm": 1.7076208591461182,
      "learning_rate": 4.997015343353585e-05,
      "loss": 3.7051,
      "step": 35
    },
    {
      "epoch": 0.87,
      "grad_norm": 1.0512803792953491,
      "learning_rate": 4.996101910454953e-05,
      "loss": 3.7443,
      "step": 40
    },
    {
      "epoch": 0.98,
      "grad_norm": 0.8518654704093933,
      "learning_rate": 4.995066821070679e-05,
      "loss": 3.5774,
      "step": 45
    },
    {
      "epoch": 1.09,
      "grad_norm": 0.9860131144523621,
      "learning_rate": 4.993910125649561e-05,
      "loss": 3.5481,
      "step": 50
    },
    {
      "epoch": 1.2,
      "grad_norm": 1.0826561450958252,
      "learning_rate": 4.992631880567301e-05,
      "loss": 3.4416,
      "step": 55
    },
    {
      "epoch": 1.31,
      "grad_norm": 0.8481911420822144,
      "learning_rate": 4.991232148123761e-05,
      "loss": 3.4859,
      "step": 60
    },
    {
      "epoch": 1.42,
      "grad_norm": 1.614713191986084,
      "learning_rate": 4.989710996539926e-05,
      "loss": 3.479,
      "step": 65
    },
    {
      "epoch": 1.53,
      "grad_norm": 0.9479525685310364,
      "learning_rate": 4.988068499954578e-05,
      "loss": 3.3769,
      "step": 70
    },
    {
      "epoch": 1.64,
      "grad_norm": 0.9738621115684509,
      "learning_rate": 4.9863047384206835e-05,
      "loss": 3.3201,
      "step": 75
    },
    {
      "epoch": 1.75,
      "grad_norm": 0.7980673909187317,
      "learning_rate": 4.984419797901491e-05,
      "loss": 3.3725,
      "step": 80
    },
    {
      "epoch": 1.86,
      "grad_norm": 0.7634841203689575,
      "learning_rate": 4.982413770266342e-05,
      "loss": 3.3198,
      "step": 85
    },
    {
      "epoch": 1.97,
      "grad_norm": 1.6109321117401123,
      "learning_rate": 4.980286753286195e-05,
      "loss": 3.3547,
      "step": 90
    },
    {
      "epoch": 2.08,
      "grad_norm": 0.8682252168655396,
      "learning_rate": 4.978038850628854e-05,
      "loss": 3.2288,
      "step": 95
    },
    {
      "epoch": 2.19,
      "grad_norm": 0.9758785963058472,
      "learning_rate": 4.975670171853926e-05,
      "loss": 3.2202,
      "step": 100
    },
    {
      "epoch": 2.3,
      "grad_norm": 1.0699541568756104,
      "learning_rate": 4.9731808324074717e-05,
      "loss": 3.2788,
      "step": 105
    },
    {
      "epoch": 2.4,
      "grad_norm": 1.163353681564331,
      "learning_rate": 4.9705709536163824e-05,
      "loss": 3.2881,
      "step": 110
    },
    {
      "epoch": 2.51,
      "grad_norm": 1.2735884189605713,
      "learning_rate": 4.96784066268247e-05,
      "loss": 3.2715,
      "step": 115
    },
    {
      "epoch": 2.62,
      "grad_norm": 0.9545009732246399,
      "learning_rate": 4.964990092676263e-05,
      "loss": 3.2181,
      "step": 120
    },
    {
      "epoch": 2.73,
      "grad_norm": 1.3793822526931763,
      "learning_rate": 4.962019382530521e-05,
      "loss": 3.1458,
      "step": 125
    },
    {
      "epoch": 2.84,
      "grad_norm": 1.2555819749832153,
      "learning_rate": 4.9589286770334654e-05,
      "loss": 3.247,
      "step": 130
    },
    {
      "epoch": 2.95,
      "grad_norm": 1.3082062005996704,
      "learning_rate": 4.9557181268217227e-05,
      "loss": 3.0994,
      "step": 135
    },
    {
      "epoch": 3.06,
      "grad_norm": 1.440890908241272,
      "learning_rate": 4.952387888372979e-05,
      "loss": 3.065,
      "step": 140
    },
    {
      "epoch": 3.17,
      "grad_norm": 1.061680555343628,
      "learning_rate": 4.94893812399836e-05,
      "loss": 3.1024,
      "step": 145
    },
    {
      "epoch": 3.28,
      "grad_norm": 1.0604188442230225,
      "learning_rate": 4.9453690018345144e-05,
      "loss": 3.1348,
      "step": 150
    },
    {
      "epoch": 3.39,
      "grad_norm": 0.9158951640129089,
      "learning_rate": 4.94168069583542e-05,
      "loss": 3.0929,
      "step": 155
    },
    {
      "epoch": 3.5,
      "grad_norm": 1.9503084421157837,
      "learning_rate": 4.937873385763908e-05,
      "loss": 3.1471,
      "step": 160
    },
    {
      "epoch": 3.61,
      "grad_norm": 1.6482278108596802,
      "learning_rate": 4.933947257182901e-05,
      "loss": 3.1082,
      "step": 165
    },
    {
      "epoch": 3.72,
      "grad_norm": 1.8090347051620483,
      "learning_rate": 4.929902501446366e-05,
      "loss": 3.0517,
      "step": 170
    },
    {
      "epoch": 3.83,
      "grad_norm": 1.215492844581604,
      "learning_rate": 4.925739315689991e-05,
      "loss": 3.0187,
      "step": 175
    },
    {
      "epoch": 3.93,
      "grad_norm": 1.305962085723877,
      "learning_rate": 4.9214579028215776e-05,
      "loss": 3.0567,
      "step": 180
    },
    {
      "epoch": 4.04,
      "grad_norm": 1.4741570949554443,
      "learning_rate": 4.917058471511149e-05,
      "loss": 3.1851,
      "step": 185
    },
    {
      "epoch": 4.15,
      "grad_norm": 1.134225606918335,
      "learning_rate": 4.912541236180779e-05,
      "loss": 3.1119,
      "step": 190
    },
    {
      "epoch": 4.26,
      "grad_norm": 1.534368634223938,
      "learning_rate": 4.907906416994146e-05,
      "loss": 2.9748,
      "step": 195
    },
    {
      "epoch": 4.37,
      "grad_norm": 1.434862732887268,
      "learning_rate": 4.9031542398457974e-05,
      "loss": 3.0311,
      "step": 200
    },
    {
      "epoch": 4.48,
      "grad_norm": 1.204943299293518,
      "learning_rate": 4.898284936350144e-05,
      "loss": 2.9843,
      "step": 205
    },
    {
      "epoch": 4.59,
      "grad_norm": 1.3439704179763794,
      "learning_rate": 4.893298743830168e-05,
      "loss": 3.0541,
      "step": 210
    },
    {
      "epoch": 4.7,
      "grad_norm": 2.6146345138549805,
      "learning_rate": 4.888195905305859e-05,
      "loss": 2.9563,
      "step": 215
    },
    {
      "epoch": 4.81,
      "grad_norm": 1.5610507726669312,
      "learning_rate": 4.882976669482367e-05,
      "loss": 2.9508,
      "step": 220
    },
    {
      "epoch": 4.92,
      "grad_norm": 4.328429698944092,
      "learning_rate": 4.877641290737884e-05,
      "loss": 2.9442,
      "step": 225
    },
    {
      "epoch": 5.03,
      "grad_norm": 1.6496464014053345,
      "learning_rate": 4.8721900291112415e-05,
      "loss": 2.9096,
      "step": 230
    },
    {
      "epoch": 5.14,
      "grad_norm": 1.2491257190704346,
      "learning_rate": 4.8666231502892415e-05,
      "loss": 2.9487,
      "step": 235
    },
    {
      "epoch": 5.25,
      "grad_norm": 2.3536081314086914,
      "learning_rate": 4.860940925593703e-05,
      "loss": 3.0078,
      "step": 240
    },
    {
      "epoch": 5.36,
      "grad_norm": 1.104355812072754,
      "learning_rate": 4.855143631968242e-05,
      "loss": 2.9357,
      "step": 245
    },
    {
      "epoch": 5.46,
      "grad_norm": 1.5648047924041748,
      "learning_rate": 4.849231551964771e-05,
      "loss": 2.9396,
      "step": 250
    },
    {
      "epoch": 5.57,
      "grad_norm": 1.7331169843673706,
      "learning_rate": 4.843204973729729e-05,
      "loss": 2.8022,
      "step": 255
    },
    {
      "epoch": 5.68,
      "grad_norm": 1.7644976377487183,
      "learning_rate": 4.837064190990036e-05,
      "loss": 2.9409,
      "step": 260
    },
    {
      "epoch": 5.79,
      "grad_norm": 1.7957509756088257,
      "learning_rate": 4.830809503038781e-05,
      "loss": 2.9713,
      "step": 265
    },
    {
      "epoch": 5.9,
      "grad_norm": 1.6274524927139282,
      "learning_rate": 4.8244412147206284e-05,
      "loss": 2.8444,
      "step": 270
    },
    {
      "epoch": 6.01,
      "grad_norm": 2.742912769317627,
      "learning_rate": 4.817959636416969e-05,
      "loss": 2.8739,
      "step": 275
    },
    {
      "epoch": 6.12,
      "grad_norm": 1.5542943477630615,
      "learning_rate": 4.8113650840307834e-05,
      "loss": 2.7814,
      "step": 280
    },
    {
      "epoch": 6.23,
      "grad_norm": 1.5975605249404907,
      "learning_rate": 4.8046578789712515e-05,
      "loss": 2.8844,
      "step": 285
    },
    {
      "epoch": 6.34,
      "grad_norm": 2.1223111152648926,
      "learning_rate": 4.797838348138086e-05,
      "loss": 2.9603,
      "step": 290
    },
    {
      "epoch": 6.45,
      "grad_norm": 3.0871102809906006,
      "learning_rate": 4.790906823905599e-05,
      "loss": 2.8727,
      "step": 295
    },
    {
      "epoch": 6.56,
      "grad_norm": 2.3582935333251953,
      "learning_rate": 4.783863644106502e-05,
      "loss": 2.7959,
      "step": 300
    },
    {
      "epoch": 6.67,
      "grad_norm": 1.5558550357818604,
      "learning_rate": 4.776709152015443e-05,
      "loss": 2.9485,
      "step": 305
    },
    {
      "epoch": 6.78,
      "grad_norm": 1.6382948160171509,
      "learning_rate": 4.769443696332272e-05,
      "loss": 2.8093,
      "step": 310
    },
    {
      "epoch": 6.89,
      "grad_norm": 2.0600132942199707,
      "learning_rate": 4.762067631165049e-05,
      "loss": 2.8527,
      "step": 315
    },
    {
      "epoch": 6.99,
      "grad_norm": 1.3371963500976562,
      "learning_rate": 4.754581316012785e-05,
      "loss": 2.887,
      "step": 320
    },
    {
      "epoch": 7.1,
      "grad_norm": 1.5286413431167603,
      "learning_rate": 4.7469851157479177e-05,
      "loss": 2.9055,
      "step": 325
    },
    {
      "epoch": 7.21,
      "grad_norm": 2.8490235805511475,
      "learning_rate": 4.7392794005985326e-05,
      "loss": 2.9468,
      "step": 330
    },
    {
      "epoch": 7.32,
      "grad_norm": 1.9624965190887451,
      "learning_rate": 4.731464546130314e-05,
      "loss": 2.7907,
      "step": 335
    },
    {
      "epoch": 7.43,
      "grad_norm": 2.847702741622925,
      "learning_rate": 4.723540933228244e-05,
      "loss": 2.8365,
      "step": 340
    },
    {
      "epoch": 7.54,
      "grad_norm": 1.4034184217453003,
      "learning_rate": 4.715508948078037e-05,
      "loss": 2.8511,
      "step": 345
    },
    {
      "epoch": 7.65,
      "grad_norm": 1.788514256477356,
      "learning_rate": 4.707368982147318e-05,
      "loss": 2.857,
      "step": 350
    },
    {
      "epoch": 7.76,
      "grad_norm": 1.9963618516921997,
      "learning_rate": 4.6991214321665414e-05,
      "loss": 2.8112,
      "step": 355
    },
    {
      "epoch": 7.87,
      "grad_norm": 1.9567127227783203,
      "learning_rate": 4.690766700109659e-05,
      "loss": 2.8015,
      "step": 360
    },
    {
      "epoch": 7.98,
      "grad_norm": 1.9742763042449951,
      "learning_rate": 4.682305193174524e-05,
      "loss": 2.8676,
      "step": 365
    },
    {
      "epoch": 8.09,
      "grad_norm": 2.649463653564453,
      "learning_rate": 4.6737373237630476e-05,
      "loss": 2.8847,
      "step": 370
    },
    {
      "epoch": 8.2,
      "grad_norm": 1.6693305969238281,
      "learning_rate": 4.665063509461097e-05,
      "loss": 2.793,
      "step": 375
    },
    {
      "epoch": 8.31,
      "grad_norm": 2.089420795440674,
      "learning_rate": 4.656284173018144e-05,
      "loss": 2.8681,
      "step": 380
    },
    {
      "epoch": 8.42,
      "grad_norm": 1.9591645002365112,
      "learning_rate": 4.6473997423266614e-05,
      "loss": 2.7823,
      "step": 385
    },
    {
      "epoch": 8.52,
      "grad_norm": 1.6575186252593994,
      "learning_rate": 4.638410650401267e-05,
      "loss": 2.7369,
      "step": 390
    },
    {
      "epoch": 8.63,
      "grad_norm": 1.923897385597229,
      "learning_rate": 4.629317335357619e-05,
      "loss": 2.7591,
      "step": 395
    },
    {
      "epoch": 8.74,
      "grad_norm": 1.9056206941604614,
      "learning_rate": 4.620120240391065e-05,
      "loss": 2.8194,
      "step": 400
    },
    {
      "epoch": 8.85,
      "grad_norm": 2.3751018047332764,
      "learning_rate": 4.610819813755038e-05,
      "loss": 2.7967,
      "step": 405
    },
    {
      "epoch": 8.96,
      "grad_norm": 1.841272234916687,
      "learning_rate": 4.601416508739211e-05,
      "loss": 2.7428,
      "step": 410
    },
    {
      "epoch": 9.07,
      "grad_norm": 2.284749746322632,
      "learning_rate": 4.591910783647404e-05,
      "loss": 2.7538,
      "step": 415
    },
    {
      "epoch": 9.18,
      "grad_norm": 3.3288071155548096,
      "learning_rate": 4.5823031017752485e-05,
      "loss": 2.7747,
      "step": 420
    },
    {
      "epoch": 9.29,
      "grad_norm": 1.6092368364334106,
      "learning_rate": 4.572593931387604e-05,
      "loss": 2.7137,
      "step": 425
    },
    {
      "epoch": 9.4,
      "grad_norm": 1.6199012994766235,
      "learning_rate": 4.562783745695738e-05,
      "loss": 2.7412,
      "step": 430
    },
    {
      "epoch": 9.51,
      "grad_norm": 2.245915174484253,
      "learning_rate": 4.5528730228342605e-05,
      "loss": 2.8198,
      "step": 435
    },
    {
      "epoch": 9.62,
      "grad_norm": 2.8612046241760254,
      "learning_rate": 4.542862245837821e-05,
      "loss": 2.7521,
      "step": 440
    },
    {
      "epoch": 9.73,
      "grad_norm": 1.9747025966644287,
      "learning_rate": 4.532751902617569e-05,
      "loss": 2.8232,
      "step": 445
    },
    {
      "epoch": 9.84,
      "grad_norm": 1.9523286819458008,
      "learning_rate": 4.522542485937369e-05,
      "loss": 2.7549,
      "step": 450
    },
    {
      "epoch": 9.95,
      "grad_norm": 2.9748637676239014,
      "learning_rate": 4.512234493389785e-05,
      "loss": 2.9136,
      "step": 455
    },
    {
      "epoch": 10.05,
      "grad_norm": 2.143521308898926,
      "learning_rate": 4.5018284273718336e-05,
      "loss": 2.7354,
      "step": 460
    },
    {
      "epoch": 10.16,
      "grad_norm": 1.9173630475997925,
      "learning_rate": 4.491324795060491e-05,
      "loss": 2.8792,
      "step": 465
    },
    {
      "epoch": 10.27,
      "grad_norm": 1.591972827911377,
      "learning_rate": 4.480724108387977e-05,
      "loss": 2.8607,
      "step": 470
    },
    {
      "epoch": 10.38,
      "grad_norm": 2.376676559448242,
      "learning_rate": 4.4700268840168045e-05,
      "loss": 2.6714,
      "step": 475
    },
    {
      "epoch": 10.49,
      "grad_norm": 1.9637905359268188,
      "learning_rate": 4.4592336433146e-05,
      "loss": 2.6977,
      "step": 480
    },
    {
      "epoch": 10.6,
      "grad_norm": 1.783913016319275,
      "learning_rate": 4.448344912328686e-05,
      "loss": 2.8091,
      "step": 485
    },
    {
      "epoch": 10.71,
      "grad_norm": 2.785374879837036,
      "learning_rate": 4.4373612217604496e-05,
      "loss": 2.6703,
      "step": 490
    },
    {
      "epoch": 10.82,
      "grad_norm": 2.0408742427825928,
      "learning_rate": 4.426283106939474e-05,
      "loss": 2.7865,
      "step": 495
    },
    {
      "epoch": 10.93,
      "grad_norm": 1.5654276609420776,
      "learning_rate": 4.415111107797445e-05,
      "loss": 2.6731,
      "step": 500
    },
    {
      "epoch": 11.04,
      "grad_norm": 2.173470973968506,
      "learning_rate": 4.403845768841842e-05,
      "loss": 2.6599,
      "step": 505
    },
    {
      "epoch": 11.15,
      "grad_norm": 3.213702440261841,
      "learning_rate": 4.3924876391293915e-05,
      "loss": 2.7658,
      "step": 510
    },
    {
      "epoch": 11.26,
      "grad_norm": 1.9662039279937744,
      "learning_rate": 4.381037272239311e-05,
      "loss": 2.6776,
      "step": 515
    },
    {
      "epoch": 11.37,
      "grad_norm": 1.6718438863754272,
      "learning_rate": 4.36949522624633e-05,
      "loss": 2.6428,
      "step": 520
    },
    {
      "epoch": 11.48,
      "grad_norm": 2.3357784748077393,
      "learning_rate": 4.357862063693486e-05,
      "loss": 2.7695,
      "step": 525
    },
    {
      "epoch": 11.58,
      "grad_norm": 2.11918306350708,
      "learning_rate": 4.3461383515647106e-05,
      "loss": 2.6464,
      "step": 530
    },
    {
      "epoch": 11.69,
      "grad_norm": 3.0614821910858154,
      "learning_rate": 4.334324661257191e-05,
      "loss": 2.7032,
      "step": 535
    },
    {
      "epoch": 11.8,
      "grad_norm": 1.9712938070297241,
      "learning_rate": 4.3224215685535294e-05,
      "loss": 2.6588,
      "step": 540
    },
    {
      "epoch": 11.91,
      "grad_norm": 2.469219207763672,
      "learning_rate": 4.3104296535936695e-05,
      "loss": 2.6809,
      "step": 545
    },
    {
      "epoch": 12.02,
      "grad_norm": 1.6632431745529175,
      "learning_rate": 4.2983495008466276e-05,
      "loss": 2.8413,
      "step": 550
    },
    {
      "epoch": 12.13,
      "grad_norm": 2.7971320152282715,
      "learning_rate": 4.2861816990820084e-05,
      "loss": 2.7944,
      "step": 555
    },
    {
      "epoch": 12.24,
      "grad_norm": 1.8295047283172607,
      "learning_rate": 4.273926841341302e-05,
      "loss": 2.6972,
      "step": 560
    },
    {
      "epoch": 12.35,
      "grad_norm": 1.9537142515182495,
      "learning_rate": 4.261585524908987e-05,
      "loss": 2.6321,
      "step": 565
    },
    {
      "epoch": 12.46,
      "grad_norm": 1.6536171436309814,
      "learning_rate": 4.249158351283414e-05,
      "loss": 2.5501,
      "step": 570
    },
    {
      "epoch": 12.57,
      "grad_norm": 2.082364320755005,
      "learning_rate": 4.2366459261474933e-05,
      "loss": 2.7045,
      "step": 575
    },
    {
      "epoch": 12.68,
      "grad_norm": 3.1343138217926025,
      "learning_rate": 4.224048859339175e-05,
      "loss": 2.6364,
      "step": 580
    },
    {
      "epoch": 12.79,
      "grad_norm": 2.99122953414917,
      "learning_rate": 4.211367764821722e-05,
      "loss": 2.6503,
      "step": 585
    },
    {
      "epoch": 12.9,
      "grad_norm": 2.024829864501953,
      "learning_rate": 4.198603260653792e-05,
      "loss": 2.6453,
      "step": 590
    },
    {
      "epoch": 13.01,
      "grad_norm": 2.1013944149017334,
      "learning_rate": 4.185755968959308e-05,
      "loss": 2.6756,
      "step": 595
    },
    {
      "epoch": 13.11,
      "grad_norm": 3.345149517059326,
      "learning_rate": 4.172826515897146e-05,
      "loss": 2.6072,
      "step": 600
    },
    {
      "epoch": 13.22,
      "grad_norm": 2.0921173095703125,
      "learning_rate": 4.1598155316306044e-05,
      "loss": 2.6775,
      "step": 605
    },
    {
      "epoch": 13.33,
      "grad_norm": 1.9780112504959106,
      "learning_rate": 4.146723650296701e-05,
      "loss": 2.6798,
      "step": 610
    },
    {
      "epoch": 13.44,
      "grad_norm": 2.3585338592529297,
      "learning_rate": 4.133551509975264e-05,
      "loss": 2.5418,
      "step": 615
    },
    {
      "epoch": 13.55,
      "grad_norm": 2.106301784515381,
      "learning_rate": 4.1202997526578276e-05,
      "loss": 2.6178,
      "step": 620
    },
    {
      "epoch": 13.66,
      "grad_norm": 2.444201707839966,
      "learning_rate": 4.1069690242163484e-05,
      "loss": 2.7604,
      "step": 625
    },
    {
      "epoch": 13.77,
      "grad_norm": 1.8051451444625854,
      "learning_rate": 4.093559974371725e-05,
      "loss": 2.7294,
      "step": 630
    },
    {
      "epoch": 13.88,
      "grad_norm": 2.9431896209716797,
      "learning_rate": 4.080073256662127e-05,
      "loss": 2.6389,
      "step": 635
    },
    {
      "epoch": 13.99,
      "grad_norm": 3.060418128967285,
      "learning_rate": 4.066509528411152e-05,
      "loss": 2.6634,
      "step": 640
    },
    {
      "epoch": 14.1,
      "grad_norm": 2.0075290203094482,
      "learning_rate": 4.052869450695776e-05,
      "loss": 2.6916,
      "step": 645
    },
    {
      "epoch": 14.21,
      "grad_norm": 2.7386865615844727,
      "learning_rate": 4.039153688314145e-05,
      "loss": 2.706,
      "step": 650
    },
    {
      "epoch": 14.32,
      "grad_norm": 2.882669687271118,
      "learning_rate": 4.02536290975317e-05,
      "loss": 2.5839,
      "step": 655
    },
    {
      "epoch": 14.43,
      "grad_norm": 2.7063851356506348,
      "learning_rate": 4.011497787155938e-05,
      "loss": 2.6345,
      "step": 660
    },
    {
      "epoch": 14.54,
      "grad_norm": 1.7728866338729858,
      "learning_rate": 3.997558996288965e-05,
      "loss": 2.6113,
      "step": 665
    },
    {
      "epoch": 14.64,
      "grad_norm": 2.0899062156677246,
      "learning_rate": 3.983547216509254e-05,
      "loss": 2.6386,
      "step": 670
    },
    {
      "epoch": 14.75,
      "grad_norm": 2.966677665710449,
      "learning_rate": 3.969463130731183e-05,
      "loss": 2.6404,
      "step": 675
    },
    {
      "epoch": 14.86,
      "grad_norm": 1.8099416494369507,
      "learning_rate": 3.955307425393224e-05,
      "loss": 2.6149,
      "step": 680
    },
    {
      "epoch": 14.97,
      "grad_norm": 1.9740931987762451,
      "learning_rate": 3.941080790424484e-05,
      "loss": 2.5981,
      "step": 685
    },
    {
      "epoch": 15.08,
      "grad_norm": 2.01714825630188,
      "learning_rate": 3.92678391921108e-05,
      "loss": 2.564,
      "step": 690
    },
    {
      "epoch": 15.19,
      "grad_norm": 2.4252054691314697,
      "learning_rate": 3.912417508562345e-05,
      "loss": 2.5909,
      "step": 695
    },
    {
      "epoch": 15.3,
      "grad_norm": 2.0233445167541504,
      "learning_rate": 3.897982258676867e-05,
      "loss": 2.5756,
      "step": 700
    },
    {
      "epoch": 15.41,
      "grad_norm": 1.9049184322357178,
      "learning_rate": 3.883478873108361e-05,
      "loss": 2.6182,
      "step": 705
    },
    {
      "epoch": 15.52,
      "grad_norm": 2.807706832885742,
      "learning_rate": 3.868908058731376e-05,
      "loss": 2.6562,
      "step": 710
    },
    {
      "epoch": 15.63,
      "grad_norm": 3.0991296768188477,
      "learning_rate": 3.85427052570685e-05,
      "loss": 2.6589,
      "step": 715
    },
    {
      "epoch": 15.74,
      "grad_norm": 3.1019067764282227,
      "learning_rate": 3.8395669874474915e-05,
      "loss": 2.5162,
      "step": 720
    },
    {
      "epoch": 15.85,
      "grad_norm": 1.8713304996490479,
      "learning_rate": 3.824798160583012e-05,
      "loss": 2.522,
      "step": 725
    },
    {
      "epoch": 15.96,
      "grad_norm": 2.5536465644836426,
      "learning_rate": 3.8099647649251986e-05,
      "loss": 2.6374,
      "step": 730
    },
    {
      "epoch": 16.07,
      "grad_norm": 2.2478325366973877,
      "learning_rate": 3.795067523432826e-05,
      "loss": 2.755,
      "step": 735
    },
    {
      "epoch": 16.17,
      "grad_norm": 1.648483395576477,
      "learning_rate": 3.780107162176429e-05,
      "loss": 2.568,
      "step": 740
    },
    {
      "epoch": 16.28,
      "grad_norm": 3.7869274616241455,
      "learning_rate": 3.765084410302909e-05,
      "loss": 2.555,
      "step": 745
    },
    {
      "epoch": 16.39,
      "grad_norm": 3.670029401779175,
      "learning_rate": 3.7500000000000003e-05,
      "loss": 2.6452,
      "step": 750
    },
    {
      "epoch": 16.5,
      "grad_norm": 2.783968925476074,
      "learning_rate": 3.7348546664605777e-05,
      "loss": 2.5808,
      "step": 755
    },
    {
      "epoch": 16.61,
      "grad_norm": 2.4255223274230957,
      "learning_rate": 3.719649147846832e-05,
      "loss": 2.5399,
      "step": 760
    },
    {
      "epoch": 16.72,
      "grad_norm": 3.265207052230835,
      "learning_rate": 3.704384185254288e-05,
      "loss": 2.6018,
      "step": 765
    },
    {
      "epoch": 16.83,
      "grad_norm": 2.012962818145752,
      "learning_rate": 3.689060522675689e-05,
      "loss": 2.5453,
      "step": 770
    },
    {
      "epoch": 16.94,
      "grad_norm": 2.975487232208252,
      "learning_rate": 3.673678906964727e-05,
      "loss": 2.6118,
      "step": 775
    },
    {
      "epoch": 17.05,
      "grad_norm": 2.437328338623047,
      "learning_rate": 3.6582400877996546e-05,
      "loss": 2.5509,
      "step": 780
    },
    {
      "epoch": 17.16,
      "grad_norm": 3.113149404525757,
      "learning_rate": 3.642744817646736e-05,
      "loss": 2.638,
      "step": 785
    },
    {
      "epoch": 17.27,
      "grad_norm": 3.129086971282959,
      "learning_rate": 3.627193851723577e-05,
      "loss": 2.5605,
      "step": 790
    },
    {
      "epoch": 17.38,
      "grad_norm": 3.720142126083374,
      "learning_rate": 3.611587947962319e-05,
      "loss": 2.558,
      "step": 795
    },
    {
      "epoch": 17.49,
      "grad_norm": 2.1802942752838135,
      "learning_rate": 3.5959278669726935e-05,
      "loss": 2.4937,
      "step": 800
    },
    {
      "epoch": 17.6,
      "grad_norm": 2.4899685382843018,
      "learning_rate": 3.580214372004956e-05,
      "loss": 2.5655,
      "step": 805
    },
    {
      "epoch": 17.7,
      "grad_norm": 2.733736991882324,
      "learning_rate": 3.564448228912682e-05,
      "loss": 2.5581,
      "step": 810
    },
    {
      "epoch": 17.81,
      "grad_norm": 2.6061012744903564,
      "learning_rate": 3.548630206115443e-05,
      "loss": 2.5646,
      "step": 815
    },
    {
      "epoch": 17.92,
      "grad_norm": 2.9398293495178223,
      "learning_rate": 3.532761074561355e-05,
      "loss": 2.64,
      "step": 820
    },
    {
      "epoch": 18.03,
      "grad_norm": 3.822050094604492,
      "learning_rate": 3.516841607689501e-05,
      "loss": 2.6143,
      "step": 825
    },
    {
      "epoch": 18.14,
      "grad_norm": 3.240427255630493,
      "learning_rate": 3.5008725813922386e-05,
      "loss": 2.5211,
      "step": 830
    },
    {
      "epoch": 18.25,
      "grad_norm": 3.0370168685913086,
      "learning_rate": 3.484854773977378e-05,
      "loss": 2.5152,
      "step": 835
    },
    {
      "epoch": 18.36,
      "grad_norm": 2.081047296524048,
      "learning_rate": 3.4687889661302576e-05,
      "loss": 2.5671,
      "step": 840
    },
    {
      "epoch": 18.47,
      "grad_norm": 4.242770195007324,
      "learning_rate": 3.452675940875686e-05,
      "loss": 2.5799,
      "step": 845
    },
    {
      "epoch": 18.58,
      "grad_norm": 5.160248279571533,
      "learning_rate": 3.436516483539781e-05,
      "loss": 2.5544,
      "step": 850
    },
    {
      "epoch": 18.69,
      "grad_norm": 2.2536041736602783,
      "learning_rate": 3.4203113817116957e-05,
      "loss": 2.4941,
      "step": 855
    },
    {
      "epoch": 18.8,
      "grad_norm": 3.1937031745910645,
      "learning_rate": 3.4040614252052305e-05,
      "loss": 2.5878,
      "step": 860
    },
    {
      "epoch": 18.91,
      "grad_norm": 2.335860252380371,
      "learning_rate": 3.387767406020343e-05,
      "loss": 2.6305,
      "step": 865
    },
    {
      "epoch": 19.02,
      "grad_norm": 2.7716612815856934,
      "learning_rate": 3.3714301183045385e-05,
      "loss": 2.5618,
      "step": 870
    },
    {
      "epoch": 19.13,
      "grad_norm": 2.643007755279541,
      "learning_rate": 3.355050358314172e-05,
      "loss": 2.4937,
      "step": 875
    },
    {
      "epoch": 19.23,
      "grad_norm": 2.4606404304504395,
      "learning_rate": 3.338628924375638e-05,
      "loss": 2.49,
      "step": 880
    },
    {
      "epoch": 19.34,
      "grad_norm": 2.234124183654785,
      "learning_rate": 3.322166616846458e-05,
      "loss": 2.4996,
      "step": 885
    },
    {
      "epoch": 19.45,
      "grad_norm": 3.4393913745880127,
      "learning_rate": 3.305664238076278e-05,
      "loss": 2.5396,
      "step": 890
    },
    {
      "epoch": 19.56,
      "grad_norm": 3.202657461166382,
      "learning_rate": 3.289122592367757e-05,
      "loss": 2.5711,
      "step": 895
    },
    {
      "epoch": 19.67,
      "grad_norm": 2.988165855407715,
      "learning_rate": 3.272542485937369e-05,
      "loss": 2.5432,
      "step": 900
    },
    {
      "epoch": 19.78,
      "grad_norm": 2.257720708847046,
      "learning_rate": 3.2559247268761115e-05,
      "loss": 2.5835,
      "step": 905
    },
    {
      "epoch": 19.89,
      "grad_norm": 2.4812440872192383,
      "learning_rate": 3.239270125110117e-05,
      "loss": 2.6011,
      "step": 910
    },
    {
      "epoch": 20.0,
      "grad_norm": 3.054295301437378,
      "learning_rate": 3.222579492361179e-05,
      "loss": 2.4526,
      "step": 915
    },
    {
      "epoch": 20.11,
      "grad_norm": 2.3782052993774414,
      "learning_rate": 3.205853642107192e-05,
      "loss": 2.4802,
      "step": 920
    },
    {
      "epoch": 20.22,
      "grad_norm": 2.7743964195251465,
      "learning_rate": 3.1890933895424976e-05,
      "loss": 2.4467,
      "step": 925
    },
    {
      "epoch": 20.33,
      "grad_norm": 2.193399429321289,
      "learning_rate": 3.172299551538164e-05,
      "loss": 2.4519,
      "step": 930
    },
    {
      "epoch": 20.44,
      "grad_norm": 2.3704917430877686,
      "learning_rate": 3.155472946602162e-05,
      "loss": 2.5026,
      "step": 935
    },
    {
      "epoch": 20.55,
      "grad_norm": 2.3345890045166016,
      "learning_rate": 3.138614394839476e-05,
      "loss": 2.5121,
      "step": 940
    },
    {
      "epoch": 20.66,
      "grad_norm": 3.6769206523895264,
      "learning_rate": 3.121724717912138e-05,
      "loss": 2.5099,
      "step": 945
    },
    {
      "epoch": 20.77,
      "grad_norm": 3.3187003135681152,
      "learning_rate": 3.104804738999169e-05,
      "loss": 2.5377,
      "step": 950
    },
    {
      "epoch": 20.87,
      "grad_norm": 2.8711345195770264,
      "learning_rate": 3.087855282756475e-05,
      "loss": 2.6043,
      "step": 955
    },
    {
      "epoch": 20.98,
      "grad_norm": 4.093137741088867,
      "learning_rate": 3.0708771752766394e-05,
      "loss": 2.5953,
      "step": 960
    },
    {
      "epoch": 21.09,
      "grad_norm": 3.2458410263061523,
      "learning_rate": 3.053871244048669e-05,
      "loss": 2.4354,
      "step": 965
    },
    {
      "epoch": 21.2,
      "grad_norm": 2.5772130489349365,
      "learning_rate": 3.0368383179176585e-05,
      "loss": 2.5405,
      "step": 970
    },
    {
      "epoch": 21.31,
      "grad_norm": 5.319322109222412,
      "learning_rate": 3.0197792270443982e-05,
      "loss": 2.4219,
      "step": 975
    },
    {
      "epoch": 21.42,
      "grad_norm": 2.3362972736358643,
      "learning_rate": 3.002694802864912e-05,
      "loss": 2.5601,
      "step": 980
    },
    {
      "epoch": 21.53,
      "grad_norm": 4.480556011199951,
      "learning_rate": 2.98558587804993e-05,
      "loss": 2.4892,
      "step": 985
    },
    {
      "epoch": 21.64,
      "grad_norm": 4.350553035736084,
      "learning_rate": 2.9684532864643122e-05,
      "loss": 2.4983,
      "step": 990
    },
    {
      "epoch": 21.75,
      "grad_norm": 2.573270797729492,
      "learning_rate": 2.9512978631264006e-05,
      "loss": 2.5522,
      "step": 995
    },
    {
      "epoch": 21.86,
      "grad_norm": 4.34506893157959,
      "learning_rate": 2.9341204441673266e-05,
      "loss": 2.5078,
      "step": 1000
    },
    {
      "epoch": 21.97,
      "grad_norm": 2.808000087738037,
      "learning_rate": 2.916921866790256e-05,
      "loss": 2.4761,
      "step": 1005
    },
    {
      "epoch": 22.08,
      "grad_norm": 2.68220591545105,
      "learning_rate": 2.8997029692295874e-05,
      "loss": 2.4597,
      "step": 1010
    },
    {
      "epoch": 22.19,
      "grad_norm": 4.24912166595459,
      "learning_rate": 2.8824645907100954e-05,
      "loss": 2.5049,
      "step": 1015
    },
    {
      "epoch": 22.3,
      "grad_norm": 3.9612669944763184,
      "learning_rate": 2.8652075714060295e-05,
      "loss": 2.5946,
      "step": 1020
    },
    {
      "epoch": 22.4,
      "grad_norm": 2.321424722671509,
      "learning_rate": 2.8479327524001636e-05,
      "loss": 2.442,
      "step": 1025
    },
    {
      "epoch": 22.51,
      "grad_norm": 4.727540493011475,
      "learning_rate": 2.8306409756428064e-05,
      "loss": 2.4459,
      "step": 1030
    },
    {
      "epoch": 22.62,
      "grad_norm": 3.8557896614074707,
      "learning_rate": 2.8133330839107608e-05,
      "loss": 2.4889,
      "step": 1035
    },
    {
      "epoch": 22.73,
      "grad_norm": 4.17302942276001,
      "learning_rate": 2.7960099207662532e-05,
      "loss": 2.5063,
      "step": 1040
    },
    {
      "epoch": 22.84,
      "grad_norm": 4.402253150939941,
      "learning_rate": 2.7786723305158136e-05,
      "loss": 2.4545,
      "step": 1045
    },
    {
      "epoch": 22.95,
      "grad_norm": 2.6420741081237793,
      "learning_rate": 2.761321158169134e-05,
      "loss": 2.4434,
      "step": 1050
    },
    {
      "epoch": 23.06,
      "grad_norm": 2.365680456161499,
      "learning_rate": 2.7439572493978736e-05,
      "loss": 2.4105,
      "step": 1055
    },
    {
      "epoch": 23.17,
      "grad_norm": 2.970679998397827,
      "learning_rate": 2.726581450494451e-05,
      "loss": 2.3983,
      "step": 1060
    },
    {
      "epoch": 23.28,
      "grad_norm": 2.354783773422241,
      "learning_rate": 2.7091946083307896e-05,
      "loss": 2.3918,
      "step": 1065
    },
    {
      "epoch": 23.39,
      "grad_norm": 3.150827646255493,
      "learning_rate": 2.6917975703170466e-05,
      "loss": 2.4884,
      "step": 1070
    },
    {
      "epoch": 23.5,
      "grad_norm": 2.7847702503204346,
      "learning_rate": 2.674391184360313e-05,
      "loss": 2.4334,
      "step": 1075
    },
    {
      "epoch": 23.61,
      "grad_norm": 3.753472089767456,
      "learning_rate": 2.656976298823284e-05,
      "loss": 2.4905,
      "step": 1080
    },
    {
      "epoch": 23.72,
      "grad_norm": 3.4611012935638428,
      "learning_rate": 2.6395537624829096e-05,
      "loss": 2.5581,
      "step": 1085
    },
    {
      "epoch": 23.83,
      "grad_norm": 2.9634251594543457,
      "learning_rate": 2.6221244244890336e-05,
      "loss": 2.4748,
      "step": 1090
    },
    {
      "epoch": 23.93,
      "grad_norm": 4.40963077545166,
      "learning_rate": 2.604689134322999e-05,
      "loss": 2.3981,
      "step": 1095
    },
    {
      "epoch": 24.04,
      "grad_norm": 2.21329665184021,
      "learning_rate": 2.587248741756253e-05,
      "loss": 2.4903,
      "step": 1100
    },
    {
      "epoch": 24.15,
      "grad_norm": 3.083101987838745,
      "learning_rate": 2.5698040968089225e-05,
      "loss": 2.4477,
      "step": 1105
    },
    {
      "epoch": 24.26,
      "grad_norm": 3.540137767791748,
      "learning_rate": 2.5523560497083926e-05,
      "loss": 2.5144,
      "step": 1110
    },
    {
      "epoch": 24.37,
      "grad_norm": 3.5205962657928467,
      "learning_rate": 2.5349054508478637e-05,
      "loss": 2.4638,
      "step": 1115
    },
    {
      "epoch": 24.48,
      "grad_norm": 3.33951735496521,
      "learning_rate": 2.517453150744904e-05,
      "loss": 2.412,
      "step": 1120
    },
    {
      "epoch": 24.59,
      "grad_norm": 3.7631826400756836,
      "learning_rate": 2.5e-05,
      "loss": 2.4557,
      "step": 1125
    },
    {
      "epoch": 24.7,
      "grad_norm": 2.6066832542419434,
      "learning_rate": 2.4825468492550964e-05,
      "loss": 2.4594,
      "step": 1130
    },
    {
      "epoch": 24.81,
      "grad_norm": 4.257205963134766,
      "learning_rate": 2.4650945491521372e-05,
      "loss": 2.4392,
      "step": 1135
    },
    {
      "epoch": 24.92,
      "grad_norm": 4.858757495880127,
      "learning_rate": 2.447643950291608e-05,
      "loss": 2.5169,
      "step": 1140
    },
    {
      "epoch": 25.03,
      "grad_norm": 4.070416450500488,
      "learning_rate": 2.4301959031910784e-05,
      "loss": 2.4997,
      "step": 1145
    },
    {
      "epoch": 25.14,
      "grad_norm": 4.7502336502075195,
      "learning_rate": 2.4127512582437485e-05,
      "loss": 2.4842,
      "step": 1150
    },
    {
      "epoch": 25.25,
      "grad_norm": 2.8164222240448,
      "learning_rate": 2.3953108656770016e-05,
      "loss": 2.3888,
      "step": 1155
    },
    {
      "epoch": 25.36,
      "grad_norm": 4.10616397857666,
      "learning_rate": 2.377875575510967e-05,
      "loss": 2.4117,
      "step": 1160
    },
    {
      "epoch": 25.46,
      "grad_norm": 3.24764084815979,
      "learning_rate": 2.3604462375170906e-05,
      "loss": 2.4483,
      "step": 1165
    },
    {
      "epoch": 25.57,
      "grad_norm": 3.2320103645324707,
      "learning_rate": 2.3430237011767167e-05,
      "loss": 2.4902,
      "step": 1170
    },
    {
      "epoch": 25.68,
      "grad_norm": 4.029989242553711,
      "learning_rate": 2.3256088156396868e-05,
      "loss": 2.4593,
      "step": 1175
    },
    {
      "epoch": 25.79,
      "grad_norm": 4.837875843048096,
      "learning_rate": 2.3082024296829536e-05,
      "loss": 2.5853,
      "step": 1180
    },
    {
      "epoch": 25.9,
      "grad_norm": 3.5902416706085205,
      "learning_rate": 2.2908053916692117e-05,
      "loss": 2.426,
      "step": 1185
    },
    {
      "epoch": 26.01,
      "grad_norm": 1.9076547622680664,
      "learning_rate": 2.2734185495055503e-05,
      "loss": 2.4524,
      "step": 1190
    },
    {
      "epoch": 26.12,
      "grad_norm": 2.7655446529388428,
      "learning_rate": 2.2560427506021266e-05,
      "loss": 2.3717,
      "step": 1195
    },
    {
      "epoch": 26.23,
      "grad_norm": 2.7638556957244873,
      "learning_rate": 2.238678841830867e-05,
      "loss": 2.4414,
      "step": 1200
    },
    {
      "epoch": 26.34,
      "grad_norm": 3.575471878051758,
      "learning_rate": 2.2213276694841866e-05,
      "loss": 2.4113,
      "step": 1205
    },
    {
      "epoch": 26.45,
      "grad_norm": 2.7083661556243896,
      "learning_rate": 2.2039900792337474e-05,
      "loss": 2.3875,
      "step": 1210
    },
    {
      "epoch": 26.56,
      "grad_norm": 4.073393821716309,
      "learning_rate": 2.186666916089239e-05,
      "loss": 2.5095,
      "step": 1215
    },
    {
      "epoch": 26.67,
      "grad_norm": 3.0759992599487305,
      "learning_rate": 2.1693590243571938e-05,
      "loss": 2.4516,
      "step": 1220
    },
    {
      "epoch": 26.78,
      "grad_norm": 4.809901237487793,
      "learning_rate": 2.1520672475998373e-05,
      "loss": 2.5,
      "step": 1225
    },
    {
      "epoch": 26.89,
      "grad_norm": 2.9302878379821777,
      "learning_rate": 2.1347924285939714e-05,
      "loss": 2.401,
      "step": 1230
    },
    {
      "epoch": 26.99,
      "grad_norm": 3.3039402961730957,
      "learning_rate": 2.117535409289905e-05,
      "loss": 2.4428,
      "step": 1235
    },
    {
      "epoch": 27.1,
      "grad_norm": 2.443258762359619,
      "learning_rate": 2.1002970307704132e-05,
      "loss": 2.3867,
      "step": 1240
    },
    {
      "epoch": 27.21,
      "grad_norm": 2.488567590713501,
      "learning_rate": 2.0830781332097446e-05,
      "loss": 2.4336,
      "step": 1245
    },
    {
      "epoch": 27.32,
      "grad_norm": 3.4901556968688965,
      "learning_rate": 2.0658795558326743e-05,
      "loss": 2.4223,
      "step": 1250
    },
    {
      "epoch": 27.43,
      "grad_norm": 3.315229892730713,
      "learning_rate": 2.0487021368736003e-05,
      "loss": 2.3541,
      "step": 1255
    },
    {
      "epoch": 27.54,
      "grad_norm": 2.951272964477539,
      "learning_rate": 2.031546713535688e-05,
      "loss": 2.4989,
      "step": 1260
    },
    {
      "epoch": 27.65,
      "grad_norm": 3.7014811038970947,
      "learning_rate": 2.0144141219500705e-05,
      "loss": 2.4731,
      "step": 1265
    },
    {
      "epoch": 27.76,
      "grad_norm": 3.1045243740081787,
      "learning_rate": 1.9973051971350888e-05,
      "loss": 2.4772,
      "step": 1270
    },
    {
      "epoch": 27.87,
      "grad_norm": 2.7251968383789062,
      "learning_rate": 1.980220772955602e-05,
      "loss": 2.5114,
      "step": 1275
    },
    {
      "epoch": 27.98,
      "grad_norm": 4.8662238121032715,
      "learning_rate": 1.963161682082342e-05,
      "loss": 2.3229,
      "step": 1280
    },
    {
      "epoch": 28.09,
      "grad_norm": 5.2037553787231445,
      "learning_rate": 1.946128755951332e-05,
      "loss": 2.349,
      "step": 1285
    },
    {
      "epoch": 28.2,
      "grad_norm": 4.288341045379639,
      "learning_rate": 1.9291228247233605e-05,
      "loss": 2.446,
      "step": 1290
    },
    {
      "epoch": 28.31,
      "grad_norm": 5.210068702697754,
      "learning_rate": 1.912144717243525e-05,
      "loss": 2.4152,
      "step": 1295
    },
    {
      "epoch": 28.42,
      "grad_norm": 2.5746872425079346,
      "learning_rate": 1.895195261000831e-05,
      "loss": 2.3549,
      "step": 1300
    },
    {
      "epoch": 28.52,
      "grad_norm": 4.216084957122803,
      "learning_rate": 1.8782752820878634e-05,
      "loss": 2.4741,
      "step": 1305
    },
    {
      "epoch": 28.63,
      "grad_norm": 5.37358283996582,
      "learning_rate": 1.8613856051605243e-05,
      "loss": 2.4001,
      "step": 1310
    },
    {
      "epoch": 28.74,
      "grad_norm": 3.6173505783081055,
      "learning_rate": 1.8445270533978388e-05,
      "loss": 2.4252,
      "step": 1315
    },
    {
      "epoch": 28.85,
      "grad_norm": 3.507467746734619,
      "learning_rate": 1.827700448461836e-05,
      "loss": 2.4224,
      "step": 1320
    },
    {
      "epoch": 28.96,
      "grad_norm": 4.072767734527588,
      "learning_rate": 1.8109066104575023e-05,
      "loss": 2.4595,
      "step": 1325
    },
    {
      "epoch": 29.07,
      "grad_norm": 4.735693454742432,
      "learning_rate": 1.7941463578928086e-05,
      "loss": 2.4019,
      "step": 1330
    },
    {
      "epoch": 29.18,
      "grad_norm": 2.8641066551208496,
      "learning_rate": 1.7774205076388206e-05,
      "loss": 2.325,
      "step": 1335
    },
    {
      "epoch": 29.29,
      "grad_norm": 3.6184470653533936,
      "learning_rate": 1.7607298748898842e-05,
      "loss": 2.4426,
      "step": 1340
    },
    {
      "epoch": 29.4,
      "grad_norm": 3.2939038276672363,
      "learning_rate": 1.744075273123889e-05,
      "loss": 2.3898,
      "step": 1345
    },
    {
      "epoch": 29.51,
      "grad_norm": 3.504326581954956,
      "learning_rate": 1.7274575140626318e-05,
      "loss": 2.3321,
      "step": 1350
    },
    {
      "epoch": 29.62,
      "grad_norm": 3.4017646312713623,
      "learning_rate": 1.7108774076322443e-05,
      "loss": 2.335,
      "step": 1355
    },
    {
      "epoch": 29.73,
      "grad_norm": 3.3392391204833984,
      "learning_rate": 1.6943357619237226e-05,
      "loss": 2.3949,
      "step": 1360
    },
    {
      "epoch": 29.84,
      "grad_norm": 2.952181339263916,
      "learning_rate": 1.677833383153542e-05,
      "loss": 2.5048,
      "step": 1365
    },
    {
      "epoch": 29.95,
      "grad_norm": 3.246154308319092,
      "learning_rate": 1.6613710756243626e-05,
      "loss": 2.3806,
      "step": 1370
    },
    {
      "epoch": 30.05,
      "grad_norm": 3.2340292930603027,
      "learning_rate": 1.6449496416858284e-05,
      "loss": 2.4398,
      "step": 1375
    },
    {
      "epoch": 30.16,
      "grad_norm": 4.7577643394470215,
      "learning_rate": 1.6285698816954624e-05,
      "loss": 2.3756,
      "step": 1380
    },
    {
      "epoch": 30.27,
      "grad_norm": 3.909966230392456,
      "learning_rate": 1.612232593979658e-05,
      "loss": 2.3604,
      "step": 1385
    },
    {
      "epoch": 30.38,
      "grad_norm": 4.4209136962890625,
      "learning_rate": 1.5959385747947698e-05,
      "loss": 2.4217,
      "step": 1390
    },
    {
      "epoch": 30.49,
      "grad_norm": 4.993971347808838,
      "learning_rate": 1.5796886182883053e-05,
      "loss": 2.4545,
      "step": 1395
    },
    {
      "epoch": 30.6,
      "grad_norm": 3.4732532501220703,
      "learning_rate": 1.56348351646022e-05,
      "loss": 2.3751,
      "step": 1400
    },
    {
      "epoch": 30.71,
      "grad_norm": 3.5623276233673096,
      "learning_rate": 1.547324059124315e-05,
      "loss": 2.3719,
      "step": 1405
    },
    {
      "epoch": 30.82,
      "grad_norm": 3.681424379348755,
      "learning_rate": 1.5312110338697426e-05,
      "loss": 2.3778,
      "step": 1410
    },
    {
      "epoch": 30.93,
      "grad_norm": 3.6803464889526367,
      "learning_rate": 1.5151452260226224e-05,
      "loss": 2.4402,
      "step": 1415
    },
    {
      "epoch": 31.04,
      "grad_norm": 4.329118251800537,
      "learning_rate": 1.4991274186077632e-05,
      "loss": 2.4458,
      "step": 1420
    },
    {
      "epoch": 31.15,
      "grad_norm": 2.180776596069336,
      "learning_rate": 1.4831583923104999e-05,
      "loss": 2.33,
      "step": 1425
    },
    {
      "epoch": 31.26,
      "grad_norm": 4.534682750701904,
      "learning_rate": 1.467238925438646e-05,
      "loss": 2.3423,
      "step": 1430
    },
    {
      "epoch": 31.37,
      "grad_norm": 4.2668070793151855,
      "learning_rate": 1.4513697938845572e-05,
      "loss": 2.4161,
      "step": 1435
    },
    {
      "epoch": 31.48,
      "grad_norm": 4.218076229095459,
      "learning_rate": 1.4355517710873184e-05,
      "loss": 2.3425,
      "step": 1440
    },
    {
      "epoch": 31.58,
      "grad_norm": 4.449320316314697,
      "learning_rate": 1.4197856279950438e-05,
      "loss": 2.4908,
      "step": 1445
    },
    {
      "epoch": 31.69,
      "grad_norm": 4.373895168304443,
      "learning_rate": 1.4040721330273062e-05,
      "loss": 2.3873,
      "step": 1450
    },
    {
      "epoch": 31.8,
      "grad_norm": 4.118370056152344,
      "learning_rate": 1.388412052037682e-05,
      "loss": 2.3552,
      "step": 1455
    },
    {
      "epoch": 31.91,
      "grad_norm": 4.808157444000244,
      "learning_rate": 1.3728061482764238e-05,
      "loss": 2.3388,
      "step": 1460
    },
    {
      "epoch": 32.02,
      "grad_norm": 3.018151044845581,
      "learning_rate": 1.3572551823532654e-05,
      "loss": 2.4209,
      "step": 1465
    },
    {
      "epoch": 32.13,
      "grad_norm": 4.922707557678223,
      "learning_rate": 1.3417599122003464e-05,
      "loss": 2.4288,
      "step": 1470
    },
    {
      "epoch": 32.24,
      "grad_norm": 3.211679220199585,
      "learning_rate": 1.3263210930352737e-05,
      "loss": 2.3288,
      "step": 1475
    },
    {
      "epoch": 32.35,
      "grad_norm": 3.102727174758911,
      "learning_rate": 1.3109394773243117e-05,
      "loss": 2.3863,
      "step": 1480
    },
    {
      "epoch": 32.46,
      "grad_norm": 4.10581111907959,
      "learning_rate": 1.2956158147457115e-05,
      "loss": 2.3387,
      "step": 1485
    },
    {
      "epoch": 32.57,
      "grad_norm": 3.439565658569336,
      "learning_rate": 1.280350852153168e-05,
      "loss": 2.398,
      "step": 1490
    },
    {
      "epoch": 32.68,
      "grad_norm": 2.9784770011901855,
      "learning_rate": 1.2651453335394231e-05,
      "loss": 2.4037,
      "step": 1495
    },
    {
      "epoch": 32.79,
      "grad_norm": 4.462327003479004,
      "learning_rate": 1.2500000000000006e-05,
      "loss": 2.412,
      "step": 1500
    },
    {
      "epoch": 32.9,
      "grad_norm": 2.3995776176452637,
      "learning_rate": 1.234915589697091e-05,
      "loss": 2.3163,
      "step": 1505
    },
    {
      "epoch": 33.01,
      "grad_norm": 4.531580448150635,
      "learning_rate": 1.2198928378235716e-05,
      "loss": 2.3718,
      "step": 1510
    },
    {
      "epoch": 33.11,
      "grad_norm": 3.5436549186706543,
      "learning_rate": 1.2049324765671749e-05,
      "loss": 2.3699,
      "step": 1515
    },
    {
      "epoch": 33.22,
      "grad_norm": 3.1035211086273193,
      "learning_rate": 1.1900352350748026e-05,
      "loss": 2.4157,
      "step": 1520
    },
    {
      "epoch": 33.33,
      "grad_norm": 3.621995210647583,
      "learning_rate": 1.175201839416988e-05,
      "loss": 2.4153,
      "step": 1525
    },
    {
      "epoch": 33.44,
      "grad_norm": 3.2188596725463867,
      "learning_rate": 1.1604330125525079e-05,
      "loss": 2.3729,
      "step": 1530
    },
    {
      "epoch": 33.55,
      "grad_norm": 4.892744064331055,
      "learning_rate": 1.1457294742931507e-05,
      "loss": 2.3117,
      "step": 1535
    },
    {
      "epoch": 33.66,
      "grad_norm": 4.766681671142578,
      "learning_rate": 1.1310919412686247e-05,
      "loss": 2.3896,
      "step": 1540
    },
    {
      "epoch": 33.77,
      "grad_norm": 4.329113006591797,
      "learning_rate": 1.11652112689164e-05,
      "loss": 2.2645,
      "step": 1545
    },
    {
      "epoch": 33.88,
      "grad_norm": 5.051767826080322,
      "learning_rate": 1.1020177413231334e-05,
      "loss": 2.3972,
      "step": 1550
    },
    {
      "epoch": 33.99,
      "grad_norm": 3.314484119415283,
      "learning_rate": 1.0875824914376553e-05,
      "loss": 2.4548,
      "step": 1555
    },
    {
      "epoch": 34.1,
      "grad_norm": 5.58599328994751,
      "learning_rate": 1.0732160807889211e-05,
      "loss": 2.4296,
      "step": 1560
    },
    {
      "epoch": 34.21,
      "grad_norm": 3.5029678344726562,
      "learning_rate": 1.058919209575517e-05,
      "loss": 2.3021,
      "step": 1565
    },
    {
      "epoch": 34.32,
      "grad_norm": 2.9447526931762695,
      "learning_rate": 1.0446925746067768e-05,
      "loss": 2.388,
      "step": 1570
    },
    {
      "epoch": 34.43,
      "grad_norm": 3.6584601402282715,
      "learning_rate": 1.0305368692688174e-05,
      "loss": 2.363,
      "step": 1575
    },
    {
      "epoch": 34.54,
      "grad_norm": 3.5150578022003174,
      "learning_rate": 1.0164527834907467e-05,
      "loss": 2.3577,
      "step": 1580
    },
    {
      "epoch": 34.64,
      "grad_norm": 3.7944748401641846,
      "learning_rate": 1.0024410037110357e-05,
      "loss": 2.2854,
      "step": 1585
    },
    {
      "epoch": 34.75,
      "grad_norm": 3.1853771209716797,
      "learning_rate": 9.88502212844063e-06,
      "loss": 2.4105,
      "step": 1590
    },
    {
      "epoch": 34.86,
      "grad_norm": 3.0395779609680176,
      "learning_rate": 9.746370902468311e-06,
      "loss": 2.3794,
      "step": 1595
    },
    {
      "epoch": 34.97,
      "grad_norm": 4.757985591888428,
      "learning_rate": 9.608463116858542e-06,
      "loss": 2.382,
      "step": 1600
    },
    {
      "epoch": 35.08,
      "grad_norm": 4.989599704742432,
      "learning_rate": 9.471305493042243e-06,
      "loss": 2.4429,
      "step": 1605
    },
    {
      "epoch": 35.19,
      "grad_norm": 6.067881107330322,
      "learning_rate": 9.334904715888495e-06,
      "loss": 2.4391,
      "step": 1610
    },
    {
      "epoch": 35.3,
      "grad_norm": 2.7555813789367676,
      "learning_rate": 9.199267433378727e-06,
      "loss": 2.3525,
      "step": 1615
    },
    {
      "epoch": 35.41,
      "grad_norm": 3.3626511096954346,
      "learning_rate": 9.064400256282757e-06,
      "loss": 2.4423,
      "step": 1620
    },
    {
      "epoch": 35.52,
      "grad_norm": 5.194301128387451,
      "learning_rate": 8.930309757836517e-06,
      "loss": 2.3169,
      "step": 1625
    },
    {
      "epoch": 35.63,
      "grad_norm": 5.018367767333984,
      "learning_rate": 8.797002473421728e-06,
      "loss": 2.3368,
      "step": 1630
    },
    {
      "epoch": 35.74,
      "grad_norm": 2.7005414962768555,
      "learning_rate": 8.664484900247363e-06,
      "loss": 2.3142,
      "step": 1635
    },
    {
      "epoch": 35.85,
      "grad_norm": 3.0990257263183594,
      "learning_rate": 8.532763497032987e-06,
      "loss": 2.3306,
      "step": 1640
    },
    {
      "epoch": 35.96,
      "grad_norm": 3.5448246002197266,
      "learning_rate": 8.40184468369396e-06,
      "loss": 2.3685,
      "step": 1645
    },
    {
      "epoch": 36.07,
      "grad_norm": 4.319765090942383,
      "learning_rate": 8.271734841028553e-06,
      "loss": 2.421,
      "step": 1650
    },
    {
      "epoch": 36.17,
      "grad_norm": 3.391658306121826,
      "learning_rate": 8.142440310406924e-06,
      "loss": 2.3029,
      "step": 1655
    },
    {
      "epoch": 36.28,
      "grad_norm": 5.254167079925537,
      "learning_rate": 8.013967393462094e-06,
      "loss": 2.3892,
      "step": 1660
    },
    {
      "epoch": 36.39,
      "grad_norm": 3.0320913791656494,
      "learning_rate": 7.886322351782783e-06,
      "loss": 2.2917,
      "step": 1665
    },
    {
      "epoch": 36.5,
      "grad_norm": 7.958362579345703,
      "learning_rate": 7.759511406608255e-06,
      "loss": 2.3014,
      "step": 1670
    },
    {
      "epoch": 36.61,
      "grad_norm": 3.225444793701172,
      "learning_rate": 7.633540738525066e-06,
      "loss": 2.4107,
      "step": 1675
    },
    {
      "epoch": 36.72,
      "grad_norm": 6.369213581085205,
      "learning_rate": 7.508416487165862e-06,
      "loss": 2.3905,
      "step": 1680
    },
    {
      "epoch": 36.83,
      "grad_norm": 3.826557159423828,
      "learning_rate": 7.384144750910133e-06,
      "loss": 2.3366,
      "step": 1685
    },
    {
      "epoch": 36.94,
      "grad_norm": 2.7093214988708496,
      "learning_rate": 7.260731586586983e-06,
      "loss": 2.3665,
      "step": 1690
    },
    {
      "epoch": 37.05,
      "grad_norm": 3.8428876399993896,
      "learning_rate": 7.138183009179922e-06,
      "loss": 2.3423,
      "step": 1695
    },
    {
      "epoch": 37.16,
      "grad_norm": 3.6955454349517822,
      "learning_rate": 7.016504991533726e-06,
      "loss": 2.3907,
      "step": 1700
    },
    {
      "epoch": 37.27,
      "grad_norm": 2.886617422103882,
      "learning_rate": 6.895703464063319e-06,
      "loss": 2.3308,
      "step": 1705
    },
    {
      "epoch": 37.38,
      "grad_norm": 2.8149960041046143,
      "learning_rate": 6.775784314464717e-06,
      "loss": 2.3623,
      "step": 1710
    },
    {
      "epoch": 37.49,
      "grad_norm": 3.6608426570892334,
      "learning_rate": 6.656753387428089e-06,
      "loss": 2.3699,
      "step": 1715
    },
    {
      "epoch": 37.6,
      "grad_norm": 5.515437602996826,
      "learning_rate": 6.538616484352902e-06,
      "loss": 2.3693,
      "step": 1720
    },
    {
      "epoch": 37.7,
      "grad_norm": 3.2513723373413086,
      "learning_rate": 6.421379363065142e-06,
      "loss": 2.4046,
      "step": 1725
    },
    {
      "epoch": 37.81,
      "grad_norm": 2.864284038543701,
      "learning_rate": 6.305047737536707e-06,
      "loss": 2.3222,
      "step": 1730
    },
    {
      "epoch": 37.92,
      "grad_norm": 4.806150436401367,
      "learning_rate": 6.189627277606894e-06,
      "loss": 2.3355,
      "step": 1735
    },
    {
      "epoch": 38.03,
      "grad_norm": 2.1834564208984375,
      "learning_rate": 6.075123608706093e-06,
      "loss": 2.3256,
      "step": 1740
    },
    {
      "epoch": 38.14,
      "grad_norm": 4.832683086395264,
      "learning_rate": 5.961542311581586e-06,
      "loss": 2.359,
      "step": 1745
    },
    {
      "epoch": 38.25,
      "grad_norm": 3.6841654777526855,
      "learning_rate": 5.848888922025553e-06,
      "loss": 2.3109,
      "step": 1750
    },
    {
      "epoch": 38.36,
      "grad_norm": 5.263666152954102,
      "learning_rate": 5.737168930605272e-06,
      "loss": 2.2801,
      "step": 1755
    },
    {
      "epoch": 38.47,
      "grad_norm": 3.595611572265625,
      "learning_rate": 5.626387782395512e-06,
      "loss": 2.3304,
      "step": 1760
    },
    {
      "epoch": 38.58,
      "grad_norm": 2.7421462535858154,
      "learning_rate": 5.5165508767131415e-06,
      "loss": 2.332,
      "step": 1765
    },
    {
      "epoch": 38.69,
      "grad_norm": 4.730967998504639,
      "learning_rate": 5.4076635668540075e-06,
      "loss": 2.3397,
      "step": 1770
    },
    {
      "epoch": 38.8,
      "grad_norm": 4.277591705322266,
      "learning_rate": 5.299731159831953e-06,
      "loss": 2.4045,
      "step": 1775
    },
    {
      "epoch": 38.91,
      "grad_norm": 3.6362502574920654,
      "learning_rate": 5.192758916120236e-06,
      "loss": 2.3381,
      "step": 1780
    },
    {
      "epoch": 39.02,
      "grad_norm": 2.87040376663208,
      "learning_rate": 5.086752049395094e-06,
      "loss": 2.3904,
      "step": 1785
    },
    {
      "epoch": 39.13,
      "grad_norm": 4.046107769012451,
      "learning_rate": 4.981715726281666e-06,
      "loss": 2.3218,
      "step": 1790
    },
    {
      "epoch": 39.23,
      "grad_norm": 4.393693923950195,
      "learning_rate": 4.877655066102149e-06,
      "loss": 2.2971,
      "step": 1795
    },
    {
      "epoch": 39.34,
      "grad_norm": 2.5045464038848877,
      "learning_rate": 4.7745751406263165e-06,
      "loss": 2.3684,
      "step": 1800
    },
    {
      "epoch": 39.45,
      "grad_norm": 3.939361572265625,
      "learning_rate": 4.672480973824311e-06,
      "loss": 2.3624,
      "step": 1805
    },
    {
      "epoch": 39.56,
      "grad_norm": 5.818185806274414,
      "learning_rate": 4.571377541621788e-06,
      "loss": 2.2794,
      "step": 1810
    },
    {
      "epoch": 39.67,
      "grad_norm": 5.122191429138184,
      "learning_rate": 4.4712697716574e-06,
      "loss": 2.3425,
      "step": 1815
    },
    {
      "epoch": 39.78,
      "grad_norm": 4.630882740020752,
      "learning_rate": 4.372162543042624e-06,
      "loss": 2.3693,
      "step": 1820
    },
    {
      "epoch": 39.89,
      "grad_norm": 4.322296142578125,
      "learning_rate": 4.274060686123959e-06,
      "loss": 2.3804,
      "step": 1825
    },
    {
      "epoch": 40.0,
      "grad_norm": 3.8837289810180664,
      "learning_rate": 4.176968982247514e-06,
      "loss": 2.3994,
      "step": 1830
    },
    {
      "epoch": 40.11,
      "grad_norm": 3.3707873821258545,
      "learning_rate": 4.08089216352596e-06,
      "loss": 2.3575,
      "step": 1835
    },
    {
      "epoch": 40.22,
      "grad_norm": 2.610413074493408,
      "learning_rate": 3.985834912607894e-06,
      "loss": 2.2865,
      "step": 1840
    },
    {
      "epoch": 40.33,
      "grad_norm": 5.209455490112305,
      "learning_rate": 3.891801862449629e-06,
      "loss": 2.3374,
      "step": 1845
    },
    {
      "epoch": 40.44,
      "grad_norm": 3.2560606002807617,
      "learning_rate": 3.798797596089351e-06,
      "loss": 2.3529,
      "step": 1850
    },
    {
      "epoch": 40.55,
      "grad_norm": 4.0685553550720215,
      "learning_rate": 3.7068266464238084e-06,
      "loss": 2.3482,
      "step": 1855
    },
    {
      "epoch": 40.66,
      "grad_norm": 2.748055934906006,
      "learning_rate": 3.6158934959873353e-06,
      "loss": 2.3418,
      "step": 1860
    },
    {
      "epoch": 40.77,
      "grad_norm": 4.1041340827941895,
      "learning_rate": 3.5260025767333893e-06,
      "loss": 2.4,
      "step": 1865
    },
    {
      "epoch": 40.87,
      "grad_norm": 3.424806833267212,
      "learning_rate": 3.4371582698185633e-06,
      "loss": 2.3698,
      "step": 1870
    },
    {
      "epoch": 40.98,
      "grad_norm": 3.5923712253570557,
      "learning_rate": 3.3493649053890326e-06,
      "loss": 2.3627,
      "step": 1875
    },
    {
      "epoch": 41.09,
      "grad_norm": 4.974755764007568,
      "learning_rate": 3.262626762369525e-06,
      "loss": 2.345,
      "step": 1880
    },
    {
      "epoch": 41.2,
      "grad_norm": 3.3741962909698486,
      "learning_rate": 3.176948068254762e-06,
      "loss": 2.3011,
      "step": 1885
    },
    {
      "epoch": 41.31,
      "grad_norm": 4.5667619705200195,
      "learning_rate": 3.092332998903416e-06,
      "loss": 2.3839,
      "step": 1890
    },
    {
      "epoch": 41.42,
      "grad_norm": 2.8570406436920166,
      "learning_rate": 3.0087856783345914e-06,
      "loss": 2.3084,
      "step": 1895
    },
    {
      "epoch": 41.53,
      "grad_norm": 3.215013265609741,
      "learning_rate": 2.9263101785268254e-06,
      "loss": 2.3527,
      "step": 1900
    },
    {
      "epoch": 41.64,
      "grad_norm": 4.224368572235107,
      "learning_rate": 2.8449105192196316e-06,
      "loss": 2.3568,
      "step": 1905
    },
    {
      "epoch": 41.75,
      "grad_norm": 4.642798900604248,
      "learning_rate": 2.764590667717562e-06,
      "loss": 2.3771,
      "step": 1910
    },
    {
      "epoch": 41.86,
      "grad_norm": 8.586105346679688,
      "learning_rate": 2.6853545386968606e-06,
      "loss": 2.344,
      "step": 1915
    },
    {
      "epoch": 41.97,
      "grad_norm": 2.7640304565429688,
      "learning_rate": 2.6072059940146775e-06,
      "loss": 2.3616,
      "step": 1920
    },
    {
      "epoch": 42.08,
      "grad_norm": 3.3341662883758545,
      "learning_rate": 2.5301488425208296e-06,
      "loss": 2.3532,
      "step": 1925
    },
    {
      "epoch": 42.19,
      "grad_norm": 3.026258945465088,
      "learning_rate": 2.454186839872158e-06,
      "loss": 2.2676,
      "step": 1930
    },
    {
      "epoch": 42.3,
      "grad_norm": 4.918916702270508,
      "learning_rate": 2.379323688349516e-06,
      "loss": 2.4168,
      "step": 1935
    },
    {
      "epoch": 42.4,
      "grad_norm": 4.303620338439941,
      "learning_rate": 2.3055630366772856e-06,
      "loss": 2.3337,
      "step": 1940
    },
    {
      "epoch": 42.51,
      "grad_norm": 4.672504425048828,
      "learning_rate": 2.2329084798455746e-06,
      "loss": 2.3693,
      "step": 1945
    },
    {
      "epoch": 42.62,
      "grad_norm": 3.744629383087158,
      "learning_rate": 2.1613635589349756e-06,
      "loss": 2.2182,
      "step": 1950
    },
    {
      "epoch": 42.73,
      "grad_norm": 4.168080806732178,
      "learning_rate": 2.0909317609440095e-06,
      "loss": 2.4096,
      "step": 1955
    },
    {
      "epoch": 42.84,
      "grad_norm": 2.930764675140381,
      "learning_rate": 2.0216165186191407e-06,
      "loss": 2.2975,
      "step": 1960
    },
    {
      "epoch": 42.95,
      "grad_norm": 4.053136825561523,
      "learning_rate": 1.95342121028749e-06,
      "loss": 2.3318,
      "step": 1965
    },
    {
      "epoch": 43.06,
      "grad_norm": 4.057053089141846,
      "learning_rate": 1.8863491596921745e-06,
      "loss": 2.3476,
      "step": 1970
    },
    {
      "epoch": 43.17,
      "grad_norm": 4.207454204559326,
      "learning_rate": 1.8204036358303173e-06,
      "loss": 2.3333,
      "step": 1975
    },
    {
      "epoch": 43.28,
      "grad_norm": 2.8967995643615723,
      "learning_rate": 1.7555878527937164e-06,
      "loss": 2.2654,
      "step": 1980
    },
    {
      "epoch": 43.39,
      "grad_norm": 3.9218568801879883,
      "learning_rate": 1.6919049696121958e-06,
      "loss": 2.4142,
      "step": 1985
    },
    {
      "epoch": 43.5,
      "grad_norm": 5.890982627868652,
      "learning_rate": 1.629358090099639e-06,
      "loss": 2.3383,
      "step": 1990
    },
    {
      "epoch": 43.61,
      "grad_norm": 4.860476493835449,
      "learning_rate": 1.5679502627027136e-06,
      "loss": 2.3953,
      "step": 1995
    },
    {
      "epoch": 43.72,
      "grad_norm": 3.7597124576568604,
      "learning_rate": 1.5076844803522922e-06,
      "loss": 2.3067,
      "step": 2000
    },
    {
      "epoch": 43.83,
      "grad_norm": 3.537597894668579,
      "learning_rate": 1.4485636803175829e-06,
      "loss": 2.2993,
      "step": 2005
    },
    {
      "epoch": 43.93,
      "grad_norm": 3.958115339279175,
      "learning_rate": 1.3905907440629752e-06,
      "loss": 2.3058,
      "step": 2010
    },
    {
      "epoch": 44.04,
      "grad_norm": 3.9737911224365234,
      "learning_rate": 1.333768497107593e-06,
      "loss": 2.3299,
      "step": 2015
    },
    {
      "epoch": 44.15,
      "grad_norm": 2.6134567260742188,
      "learning_rate": 1.2780997088875869e-06,
      "loss": 2.3595,
      "step": 2020
    },
    {
      "epoch": 44.26,
      "grad_norm": 4.5460686683654785,
      "learning_rate": 1.2235870926211619e-06,
      "loss": 2.349,
      "step": 2025
    },
    {
      "epoch": 44.37,
      "grad_norm": 3.8527047634124756,
      "learning_rate": 1.170233305176327e-06,
      "loss": 2.3535,
      "step": 2030
    },
    {
      "epoch": 44.48,
      "grad_norm": 3.546884298324585,
      "learning_rate": 1.1180409469414094e-06,
      "loss": 2.2033,
      "step": 2035
    },
    {
      "epoch": 44.59,
      "grad_norm": 3.3972690105438232,
      "learning_rate": 1.067012561698319e-06,
      "loss": 2.3627,
      "step": 2040
    },
    {
      "epoch": 44.7,
      "grad_norm": 4.220953941345215,
      "learning_rate": 1.0171506364985622e-06,
      "loss": 2.3299,
      "step": 2045
    },
    {
      "epoch": 44.81,
      "grad_norm": 4.301341533660889,
      "learning_rate": 9.684576015420278e-07,
      "loss": 2.348,
      "step": 2050
    },
    {
      "epoch": 44.92,
      "grad_norm": 3.2165117263793945,
      "learning_rate": 9.209358300585474e-07,
      "loss": 2.3602,
      "step": 2055
    },
    {
      "epoch": 45.03,
      "grad_norm": 6.0050368309021,
      "learning_rate": 8.745876381922147e-07,
      "loss": 2.3896,
      "step": 2060
    },
    {
      "epoch": 45.14,
      "grad_norm": 3.2420430183410645,
      "learning_rate": 8.294152848885157e-07,
      "loss": 2.3397,
      "step": 2065
    },
    {
      "epoch": 45.25,
      "grad_norm": 3.3606159687042236,
      "learning_rate": 7.854209717842231e-07,
      "loss": 2.3016,
      "step": 2070
    },
    {
      "epoch": 45.36,
      "grad_norm": 3.433988332748413,
      "learning_rate": 7.426068431000882e-07,
      "loss": 2.3098,
      "step": 2075
    },
    {
      "epoch": 45.46,
      "grad_norm": 3.761014223098755,
      "learning_rate": 7.009749855363456e-07,
      "loss": 2.431,
      "step": 2080
    },
    {
      "epoch": 45.57,
      "grad_norm": 2.838470220565796,
      "learning_rate": 6.605274281709928e-07,
      "loss": 2.3712,
      "step": 2085
    },
    {
      "epoch": 45.68,
      "grad_norm": 5.792356967926025,
      "learning_rate": 6.212661423609184e-07,
      "loss": 2.3635,
      "step": 2090
    },
    {
      "epoch": 45.79,
      "grad_norm": 2.8011374473571777,
      "learning_rate": 5.83193041645802e-07,
      "loss": 2.3244,
      "step": 2095
    },
    {
      "epoch": 45.9,
      "grad_norm": 4.375436305999756,
      "learning_rate": 5.463099816548579e-07,
      "loss": 2.2748,
      "step": 2100
    },
    {
      "epoch": 46.01,
      "grad_norm": 7.342792987823486,
      "learning_rate": 5.106187600163987e-07,
      "loss": 2.316,
      "step": 2105
    },
    {
      "epoch": 46.12,
      "grad_norm": 3.4065968990325928,
      "learning_rate": 4.7612111627021175e-07,
      "loss": 2.325,
      "step": 2110
    },
    {
      "epoch": 46.23,
      "grad_norm": 2.4625768661499023,
      "learning_rate": 4.4281873178278475e-07,
      "loss": 2.3149,
      "step": 2115
    },
    {
      "epoch": 46.34,
      "grad_norm": 3.316568374633789,
      "learning_rate": 4.107132296653549e-07,
      "loss": 2.316,
      "step": 2120
    },
    {
      "epoch": 46.45,
      "grad_norm": 3.072434186935425,
      "learning_rate": 3.7980617469479953e-07,
      "loss": 2.2647,
      "step": 2125
    },
    {
      "epoch": 46.56,
      "grad_norm": 5.161874771118164,
      "learning_rate": 3.5009907323737825e-07,
      "loss": 2.3686,
      "step": 2130
    },
    {
      "epoch": 46.67,
      "grad_norm": 4.3369035720825195,
      "learning_rate": 3.215933731753024e-07,
      "loss": 2.4041,
      "step": 2135
    },
    {
      "epoch": 46.78,
      "grad_norm": 3.521143913269043,
      "learning_rate": 2.942904638361804e-07,
      "loss": 2.3183,
      "step": 2140
    },
    {
      "epoch": 46.89,
      "grad_norm": 3.100085735321045,
      "learning_rate": 2.681916759252917e-07,
      "loss": 2.3838,
      "step": 2145
    },
    {
      "epoch": 46.99,
      "grad_norm": 4.608964443206787,
      "learning_rate": 2.4329828146074095e-07,
      "loss": 2.2958,
      "step": 2150
    },
    {
      "epoch": 47.1,
      "grad_norm": 3.453629970550537,
      "learning_rate": 2.1961149371145795e-07,
      "loss": 2.2918,
      "step": 2155
    },
    {
      "epoch": 47.21,
      "grad_norm": 4.721307754516602,
      "learning_rate": 1.9713246713805588e-07,
      "loss": 2.4082,
      "step": 2160
    },
    {
      "epoch": 47.32,
      "grad_norm": 2.6232786178588867,
      "learning_rate": 1.7586229733657644e-07,
      "loss": 2.2901,
      "step": 2165
    },
    {
      "epoch": 47.43,
      "grad_norm": 5.266059398651123,
      "learning_rate": 1.5580202098509077e-07,
      "loss": 2.3297,
      "step": 2170
    },
    {
      "epoch": 47.54,
      "grad_norm": 5.092928886413574,
      "learning_rate": 1.3695261579316777e-07,
      "loss": 2.2848,
      "step": 2175
    },
    {
      "epoch": 47.65,
      "grad_norm": 4.356197357177734,
      "learning_rate": 1.193150004542204e-07,
      "loss": 2.4179,
      "step": 2180
    },
    {
      "epoch": 47.76,
      "grad_norm": 6.88424825668335,
      "learning_rate": 1.0289003460074165e-07,
      "loss": 2.3231,
      "step": 2185
    },
    {
      "epoch": 47.87,
      "grad_norm": 7.060300350189209,
      "learning_rate": 8.767851876239074e-08,
      "loss": 2.3839,
      "step": 2190
    },
    {
      "epoch": 47.98,
      "grad_norm": 4.322144031524658,
      "learning_rate": 7.368119432699383e-08,
      "loss": 2.3096,
      "step": 2195
    },
    {
      "epoch": 48.09,
      "grad_norm": 4.225009918212891,
      "learning_rate": 6.089874350439506e-08,
      "loss": 2.3688,
      "step": 2200
    },
    {
      "epoch": 48.2,
      "grad_norm": 3.188342332839966,
      "learning_rate": 4.9331789293211026e-08,
      "loss": 2.3223,
      "step": 2205
    },
    {
      "epoch": 48.31,
      "grad_norm": 6.990561485290527,
      "learning_rate": 3.8980895450474455e-08,
      "loss": 2.3495,
      "step": 2210
    },
    {
      "epoch": 48.42,
      "grad_norm": 3.033914804458618,
      "learning_rate": 2.9846566464150626e-08,
      "loss": 2.3507,
      "step": 2215
    },
    {
      "epoch": 48.52,
      "grad_norm": 3.719625234603882,
      "learning_rate": 2.192924752854042e-08,
      "loss": 2.2821,
      "step": 2220
    },
    {
      "epoch": 48.63,
      "grad_norm": 3.3597283363342285,
      "learning_rate": 1.522932452260595e-08,
      "loss": 2.36,
      "step": 2225
    },
    {
      "epoch": 48.74,
      "grad_norm": 6.524499893188477,
      "learning_rate": 9.747123991141194e-09,
      "loss": 2.3803,
      "step": 2230
    },
    {
      "epoch": 48.85,
      "grad_norm": 4.239129066467285,
      "learning_rate": 5.48291312886251e-09,
      "loss": 2.3438,
      "step": 2235
    },
    {
      "epoch": 48.96,
      "grad_norm": 4.494908332824707,
      "learning_rate": 2.4368997673940297e-09,
      "loss": 2.2367,
      "step": 2240
    },
    {
      "epoch": 49.07,
      "grad_norm": 3.7638959884643555,
      "learning_rate": 6.092323651313292e-10,
      "loss": 2.2936,
      "step": 2245
    },
    {
      "epoch": 49.18,
      "grad_norm": 5.652186393737793,
      "learning_rate": 0.0,
      "loss": 2.3999,
      "step": 2250
    },
    {
      "epoch": 49.18,
      "step": 2250,
      "total_flos": 1.0345613322538844e+18,
      "train_loss": 2.576482096354167,
      "train_runtime": 8370.3812,
      "train_samples_per_second": 4.361,
      "train_steps_per_second": 0.269
    }
  ],
  "logging_steps": 5,
  "max_steps": 2250,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 50,
  "save_steps": 100,
  "total_flos": 1.0345613322538844e+18,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
