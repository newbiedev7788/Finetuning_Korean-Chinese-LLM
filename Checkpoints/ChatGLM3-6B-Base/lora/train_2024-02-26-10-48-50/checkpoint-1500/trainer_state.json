{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 32.78688524590164,
  "eval_steps": 500,
  "global_step": 1500,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.11,
      "grad_norm": 0.4190762937068939,
      "learning_rate": 4.999939076763487e-05,
      "loss": 4.0813,
      "step": 5
    },
    {
      "epoch": 0.22,
      "grad_norm": 0.5682299137115479,
      "learning_rate": 4.999756310023261e-05,
      "loss": 3.8759,
      "step": 10
    },
    {
      "epoch": 0.33,
      "grad_norm": 0.6293856501579285,
      "learning_rate": 4.999451708687114e-05,
      "loss": 3.9084,
      "step": 15
    },
    {
      "epoch": 0.44,
      "grad_norm": 0.8758161664009094,
      "learning_rate": 4.999025287600886e-05,
      "loss": 3.8482,
      "step": 20
    },
    {
      "epoch": 0.55,
      "grad_norm": 1.1061590909957886,
      "learning_rate": 4.99847706754774e-05,
      "loss": 3.9143,
      "step": 25
    },
    {
      "epoch": 0.66,
      "grad_norm": 1.0389021635055542,
      "learning_rate": 4.997807075247146e-05,
      "loss": 3.8069,
      "step": 30
    },
    {
      "epoch": 0.77,
      "grad_norm": 1.7076208591461182,
      "learning_rate": 4.997015343353585e-05,
      "loss": 3.7051,
      "step": 35
    },
    {
      "epoch": 0.87,
      "grad_norm": 1.0512803792953491,
      "learning_rate": 4.996101910454953e-05,
      "loss": 3.7443,
      "step": 40
    },
    {
      "epoch": 0.98,
      "grad_norm": 0.8518654704093933,
      "learning_rate": 4.995066821070679e-05,
      "loss": 3.5774,
      "step": 45
    },
    {
      "epoch": 1.09,
      "grad_norm": 0.9860131144523621,
      "learning_rate": 4.993910125649561e-05,
      "loss": 3.5481,
      "step": 50
    },
    {
      "epoch": 1.2,
      "grad_norm": 1.0826561450958252,
      "learning_rate": 4.992631880567301e-05,
      "loss": 3.4416,
      "step": 55
    },
    {
      "epoch": 1.31,
      "grad_norm": 0.8481911420822144,
      "learning_rate": 4.991232148123761e-05,
      "loss": 3.4859,
      "step": 60
    },
    {
      "epoch": 1.42,
      "grad_norm": 1.614713191986084,
      "learning_rate": 4.989710996539926e-05,
      "loss": 3.479,
      "step": 65
    },
    {
      "epoch": 1.53,
      "grad_norm": 0.9479525685310364,
      "learning_rate": 4.988068499954578e-05,
      "loss": 3.3769,
      "step": 70
    },
    {
      "epoch": 1.64,
      "grad_norm": 0.9738621115684509,
      "learning_rate": 4.9863047384206835e-05,
      "loss": 3.3201,
      "step": 75
    },
    {
      "epoch": 1.75,
      "grad_norm": 0.7980673909187317,
      "learning_rate": 4.984419797901491e-05,
      "loss": 3.3725,
      "step": 80
    },
    {
      "epoch": 1.86,
      "grad_norm": 0.7634841203689575,
      "learning_rate": 4.982413770266342e-05,
      "loss": 3.3198,
      "step": 85
    },
    {
      "epoch": 1.97,
      "grad_norm": 1.6109321117401123,
      "learning_rate": 4.980286753286195e-05,
      "loss": 3.3547,
      "step": 90
    },
    {
      "epoch": 2.08,
      "grad_norm": 0.8682252168655396,
      "learning_rate": 4.978038850628854e-05,
      "loss": 3.2288,
      "step": 95
    },
    {
      "epoch": 2.19,
      "grad_norm": 0.9758785963058472,
      "learning_rate": 4.975670171853926e-05,
      "loss": 3.2202,
      "step": 100
    },
    {
      "epoch": 2.3,
      "grad_norm": 1.0699541568756104,
      "learning_rate": 4.9731808324074717e-05,
      "loss": 3.2788,
      "step": 105
    },
    {
      "epoch": 2.4,
      "grad_norm": 1.163353681564331,
      "learning_rate": 4.9705709536163824e-05,
      "loss": 3.2881,
      "step": 110
    },
    {
      "epoch": 2.51,
      "grad_norm": 1.2735884189605713,
      "learning_rate": 4.96784066268247e-05,
      "loss": 3.2715,
      "step": 115
    },
    {
      "epoch": 2.62,
      "grad_norm": 0.9545009732246399,
      "learning_rate": 4.964990092676263e-05,
      "loss": 3.2181,
      "step": 120
    },
    {
      "epoch": 2.73,
      "grad_norm": 1.3793822526931763,
      "learning_rate": 4.962019382530521e-05,
      "loss": 3.1458,
      "step": 125
    },
    {
      "epoch": 2.84,
      "grad_norm": 1.2555819749832153,
      "learning_rate": 4.9589286770334654e-05,
      "loss": 3.247,
      "step": 130
    },
    {
      "epoch": 2.95,
      "grad_norm": 1.3082062005996704,
      "learning_rate": 4.9557181268217227e-05,
      "loss": 3.0994,
      "step": 135
    },
    {
      "epoch": 3.06,
      "grad_norm": 1.440890908241272,
      "learning_rate": 4.952387888372979e-05,
      "loss": 3.065,
      "step": 140
    },
    {
      "epoch": 3.17,
      "grad_norm": 1.061680555343628,
      "learning_rate": 4.94893812399836e-05,
      "loss": 3.1024,
      "step": 145
    },
    {
      "epoch": 3.28,
      "grad_norm": 1.0604188442230225,
      "learning_rate": 4.9453690018345144e-05,
      "loss": 3.1348,
      "step": 150
    },
    {
      "epoch": 3.39,
      "grad_norm": 0.9158951640129089,
      "learning_rate": 4.94168069583542e-05,
      "loss": 3.0929,
      "step": 155
    },
    {
      "epoch": 3.5,
      "grad_norm": 1.9503084421157837,
      "learning_rate": 4.937873385763908e-05,
      "loss": 3.1471,
      "step": 160
    },
    {
      "epoch": 3.61,
      "grad_norm": 1.6482278108596802,
      "learning_rate": 4.933947257182901e-05,
      "loss": 3.1082,
      "step": 165
    },
    {
      "epoch": 3.72,
      "grad_norm": 1.8090347051620483,
      "learning_rate": 4.929902501446366e-05,
      "loss": 3.0517,
      "step": 170
    },
    {
      "epoch": 3.83,
      "grad_norm": 1.215492844581604,
      "learning_rate": 4.925739315689991e-05,
      "loss": 3.0187,
      "step": 175
    },
    {
      "epoch": 3.93,
      "grad_norm": 1.305962085723877,
      "learning_rate": 4.9214579028215776e-05,
      "loss": 3.0567,
      "step": 180
    },
    {
      "epoch": 4.04,
      "grad_norm": 1.4741570949554443,
      "learning_rate": 4.917058471511149e-05,
      "loss": 3.1851,
      "step": 185
    },
    {
      "epoch": 4.15,
      "grad_norm": 1.134225606918335,
      "learning_rate": 4.912541236180779e-05,
      "loss": 3.1119,
      "step": 190
    },
    {
      "epoch": 4.26,
      "grad_norm": 1.534368634223938,
      "learning_rate": 4.907906416994146e-05,
      "loss": 2.9748,
      "step": 195
    },
    {
      "epoch": 4.37,
      "grad_norm": 1.434862732887268,
      "learning_rate": 4.9031542398457974e-05,
      "loss": 3.0311,
      "step": 200
    },
    {
      "epoch": 4.48,
      "grad_norm": 1.204943299293518,
      "learning_rate": 4.898284936350144e-05,
      "loss": 2.9843,
      "step": 205
    },
    {
      "epoch": 4.59,
      "grad_norm": 1.3439704179763794,
      "learning_rate": 4.893298743830168e-05,
      "loss": 3.0541,
      "step": 210
    },
    {
      "epoch": 4.7,
      "grad_norm": 2.6146345138549805,
      "learning_rate": 4.888195905305859e-05,
      "loss": 2.9563,
      "step": 215
    },
    {
      "epoch": 4.81,
      "grad_norm": 1.5610507726669312,
      "learning_rate": 4.882976669482367e-05,
      "loss": 2.9508,
      "step": 220
    },
    {
      "epoch": 4.92,
      "grad_norm": 4.328429698944092,
      "learning_rate": 4.877641290737884e-05,
      "loss": 2.9442,
      "step": 225
    },
    {
      "epoch": 5.03,
      "grad_norm": 1.6496464014053345,
      "learning_rate": 4.8721900291112415e-05,
      "loss": 2.9096,
      "step": 230
    },
    {
      "epoch": 5.14,
      "grad_norm": 1.2491257190704346,
      "learning_rate": 4.8666231502892415e-05,
      "loss": 2.9487,
      "step": 235
    },
    {
      "epoch": 5.25,
      "grad_norm": 2.3536081314086914,
      "learning_rate": 4.860940925593703e-05,
      "loss": 3.0078,
      "step": 240
    },
    {
      "epoch": 5.36,
      "grad_norm": 1.104355812072754,
      "learning_rate": 4.855143631968242e-05,
      "loss": 2.9357,
      "step": 245
    },
    {
      "epoch": 5.46,
      "grad_norm": 1.5648047924041748,
      "learning_rate": 4.849231551964771e-05,
      "loss": 2.9396,
      "step": 250
    },
    {
      "epoch": 5.57,
      "grad_norm": 1.7331169843673706,
      "learning_rate": 4.843204973729729e-05,
      "loss": 2.8022,
      "step": 255
    },
    {
      "epoch": 5.68,
      "grad_norm": 1.7644976377487183,
      "learning_rate": 4.837064190990036e-05,
      "loss": 2.9409,
      "step": 260
    },
    {
      "epoch": 5.79,
      "grad_norm": 1.7957509756088257,
      "learning_rate": 4.830809503038781e-05,
      "loss": 2.9713,
      "step": 265
    },
    {
      "epoch": 5.9,
      "grad_norm": 1.6274524927139282,
      "learning_rate": 4.8244412147206284e-05,
      "loss": 2.8444,
      "step": 270
    },
    {
      "epoch": 6.01,
      "grad_norm": 2.742912769317627,
      "learning_rate": 4.817959636416969e-05,
      "loss": 2.8739,
      "step": 275
    },
    {
      "epoch": 6.12,
      "grad_norm": 1.5542943477630615,
      "learning_rate": 4.8113650840307834e-05,
      "loss": 2.7814,
      "step": 280
    },
    {
      "epoch": 6.23,
      "grad_norm": 1.5975605249404907,
      "learning_rate": 4.8046578789712515e-05,
      "loss": 2.8844,
      "step": 285
    },
    {
      "epoch": 6.34,
      "grad_norm": 2.1223111152648926,
      "learning_rate": 4.797838348138086e-05,
      "loss": 2.9603,
      "step": 290
    },
    {
      "epoch": 6.45,
      "grad_norm": 3.0871102809906006,
      "learning_rate": 4.790906823905599e-05,
      "loss": 2.8727,
      "step": 295
    },
    {
      "epoch": 6.56,
      "grad_norm": 2.3582935333251953,
      "learning_rate": 4.783863644106502e-05,
      "loss": 2.7959,
      "step": 300
    },
    {
      "epoch": 6.67,
      "grad_norm": 1.5558550357818604,
      "learning_rate": 4.776709152015443e-05,
      "loss": 2.9485,
      "step": 305
    },
    {
      "epoch": 6.78,
      "grad_norm": 1.6382948160171509,
      "learning_rate": 4.769443696332272e-05,
      "loss": 2.8093,
      "step": 310
    },
    {
      "epoch": 6.89,
      "grad_norm": 2.0600132942199707,
      "learning_rate": 4.762067631165049e-05,
      "loss": 2.8527,
      "step": 315
    },
    {
      "epoch": 6.99,
      "grad_norm": 1.3371963500976562,
      "learning_rate": 4.754581316012785e-05,
      "loss": 2.887,
      "step": 320
    },
    {
      "epoch": 7.1,
      "grad_norm": 1.5286413431167603,
      "learning_rate": 4.7469851157479177e-05,
      "loss": 2.9055,
      "step": 325
    },
    {
      "epoch": 7.21,
      "grad_norm": 2.8490235805511475,
      "learning_rate": 4.7392794005985326e-05,
      "loss": 2.9468,
      "step": 330
    },
    {
      "epoch": 7.32,
      "grad_norm": 1.9624965190887451,
      "learning_rate": 4.731464546130314e-05,
      "loss": 2.7907,
      "step": 335
    },
    {
      "epoch": 7.43,
      "grad_norm": 2.847702741622925,
      "learning_rate": 4.723540933228244e-05,
      "loss": 2.8365,
      "step": 340
    },
    {
      "epoch": 7.54,
      "grad_norm": 1.4034184217453003,
      "learning_rate": 4.715508948078037e-05,
      "loss": 2.8511,
      "step": 345
    },
    {
      "epoch": 7.65,
      "grad_norm": 1.788514256477356,
      "learning_rate": 4.707368982147318e-05,
      "loss": 2.857,
      "step": 350
    },
    {
      "epoch": 7.76,
      "grad_norm": 1.9963618516921997,
      "learning_rate": 4.6991214321665414e-05,
      "loss": 2.8112,
      "step": 355
    },
    {
      "epoch": 7.87,
      "grad_norm": 1.9567127227783203,
      "learning_rate": 4.690766700109659e-05,
      "loss": 2.8015,
      "step": 360
    },
    {
      "epoch": 7.98,
      "grad_norm": 1.9742763042449951,
      "learning_rate": 4.682305193174524e-05,
      "loss": 2.8676,
      "step": 365
    },
    {
      "epoch": 8.09,
      "grad_norm": 2.649463653564453,
      "learning_rate": 4.6737373237630476e-05,
      "loss": 2.8847,
      "step": 370
    },
    {
      "epoch": 8.2,
      "grad_norm": 1.6693305969238281,
      "learning_rate": 4.665063509461097e-05,
      "loss": 2.793,
      "step": 375
    },
    {
      "epoch": 8.31,
      "grad_norm": 2.089420795440674,
      "learning_rate": 4.656284173018144e-05,
      "loss": 2.8681,
      "step": 380
    },
    {
      "epoch": 8.42,
      "grad_norm": 1.9591645002365112,
      "learning_rate": 4.6473997423266614e-05,
      "loss": 2.7823,
      "step": 385
    },
    {
      "epoch": 8.52,
      "grad_norm": 1.6575186252593994,
      "learning_rate": 4.638410650401267e-05,
      "loss": 2.7369,
      "step": 390
    },
    {
      "epoch": 8.63,
      "grad_norm": 1.923897385597229,
      "learning_rate": 4.629317335357619e-05,
      "loss": 2.7591,
      "step": 395
    },
    {
      "epoch": 8.74,
      "grad_norm": 1.9056206941604614,
      "learning_rate": 4.620120240391065e-05,
      "loss": 2.8194,
      "step": 400
    },
    {
      "epoch": 8.85,
      "grad_norm": 2.3751018047332764,
      "learning_rate": 4.610819813755038e-05,
      "loss": 2.7967,
      "step": 405
    },
    {
      "epoch": 8.96,
      "grad_norm": 1.841272234916687,
      "learning_rate": 4.601416508739211e-05,
      "loss": 2.7428,
      "step": 410
    },
    {
      "epoch": 9.07,
      "grad_norm": 2.284749746322632,
      "learning_rate": 4.591910783647404e-05,
      "loss": 2.7538,
      "step": 415
    },
    {
      "epoch": 9.18,
      "grad_norm": 3.3288071155548096,
      "learning_rate": 4.5823031017752485e-05,
      "loss": 2.7747,
      "step": 420
    },
    {
      "epoch": 9.29,
      "grad_norm": 1.6092368364334106,
      "learning_rate": 4.572593931387604e-05,
      "loss": 2.7137,
      "step": 425
    },
    {
      "epoch": 9.4,
      "grad_norm": 1.6199012994766235,
      "learning_rate": 4.562783745695738e-05,
      "loss": 2.7412,
      "step": 430
    },
    {
      "epoch": 9.51,
      "grad_norm": 2.245915174484253,
      "learning_rate": 4.5528730228342605e-05,
      "loss": 2.8198,
      "step": 435
    },
    {
      "epoch": 9.62,
      "grad_norm": 2.8612046241760254,
      "learning_rate": 4.542862245837821e-05,
      "loss": 2.7521,
      "step": 440
    },
    {
      "epoch": 9.73,
      "grad_norm": 1.9747025966644287,
      "learning_rate": 4.532751902617569e-05,
      "loss": 2.8232,
      "step": 445
    },
    {
      "epoch": 9.84,
      "grad_norm": 1.9523286819458008,
      "learning_rate": 4.522542485937369e-05,
      "loss": 2.7549,
      "step": 450
    },
    {
      "epoch": 9.95,
      "grad_norm": 2.9748637676239014,
      "learning_rate": 4.512234493389785e-05,
      "loss": 2.9136,
      "step": 455
    },
    {
      "epoch": 10.05,
      "grad_norm": 2.143521308898926,
      "learning_rate": 4.5018284273718336e-05,
      "loss": 2.7354,
      "step": 460
    },
    {
      "epoch": 10.16,
      "grad_norm": 1.9173630475997925,
      "learning_rate": 4.491324795060491e-05,
      "loss": 2.8792,
      "step": 465
    },
    {
      "epoch": 10.27,
      "grad_norm": 1.591972827911377,
      "learning_rate": 4.480724108387977e-05,
      "loss": 2.8607,
      "step": 470
    },
    {
      "epoch": 10.38,
      "grad_norm": 2.376676559448242,
      "learning_rate": 4.4700268840168045e-05,
      "loss": 2.6714,
      "step": 475
    },
    {
      "epoch": 10.49,
      "grad_norm": 1.9637905359268188,
      "learning_rate": 4.4592336433146e-05,
      "loss": 2.6977,
      "step": 480
    },
    {
      "epoch": 10.6,
      "grad_norm": 1.783913016319275,
      "learning_rate": 4.448344912328686e-05,
      "loss": 2.8091,
      "step": 485
    },
    {
      "epoch": 10.71,
      "grad_norm": 2.785374879837036,
      "learning_rate": 4.4373612217604496e-05,
      "loss": 2.6703,
      "step": 490
    },
    {
      "epoch": 10.82,
      "grad_norm": 2.0408742427825928,
      "learning_rate": 4.426283106939474e-05,
      "loss": 2.7865,
      "step": 495
    },
    {
      "epoch": 10.93,
      "grad_norm": 1.5654276609420776,
      "learning_rate": 4.415111107797445e-05,
      "loss": 2.6731,
      "step": 500
    },
    {
      "epoch": 11.04,
      "grad_norm": 2.173470973968506,
      "learning_rate": 4.403845768841842e-05,
      "loss": 2.6599,
      "step": 505
    },
    {
      "epoch": 11.15,
      "grad_norm": 3.213702440261841,
      "learning_rate": 4.3924876391293915e-05,
      "loss": 2.7658,
      "step": 510
    },
    {
      "epoch": 11.26,
      "grad_norm": 1.9662039279937744,
      "learning_rate": 4.381037272239311e-05,
      "loss": 2.6776,
      "step": 515
    },
    {
      "epoch": 11.37,
      "grad_norm": 1.6718438863754272,
      "learning_rate": 4.36949522624633e-05,
      "loss": 2.6428,
      "step": 520
    },
    {
      "epoch": 11.48,
      "grad_norm": 2.3357784748077393,
      "learning_rate": 4.357862063693486e-05,
      "loss": 2.7695,
      "step": 525
    },
    {
      "epoch": 11.58,
      "grad_norm": 2.11918306350708,
      "learning_rate": 4.3461383515647106e-05,
      "loss": 2.6464,
      "step": 530
    },
    {
      "epoch": 11.69,
      "grad_norm": 3.0614821910858154,
      "learning_rate": 4.334324661257191e-05,
      "loss": 2.7032,
      "step": 535
    },
    {
      "epoch": 11.8,
      "grad_norm": 1.9712938070297241,
      "learning_rate": 4.3224215685535294e-05,
      "loss": 2.6588,
      "step": 540
    },
    {
      "epoch": 11.91,
      "grad_norm": 2.469219207763672,
      "learning_rate": 4.3104296535936695e-05,
      "loss": 2.6809,
      "step": 545
    },
    {
      "epoch": 12.02,
      "grad_norm": 1.6632431745529175,
      "learning_rate": 4.2983495008466276e-05,
      "loss": 2.8413,
      "step": 550
    },
    {
      "epoch": 12.13,
      "grad_norm": 2.7971320152282715,
      "learning_rate": 4.2861816990820084e-05,
      "loss": 2.7944,
      "step": 555
    },
    {
      "epoch": 12.24,
      "grad_norm": 1.8295047283172607,
      "learning_rate": 4.273926841341302e-05,
      "loss": 2.6972,
      "step": 560
    },
    {
      "epoch": 12.35,
      "grad_norm": 1.9537142515182495,
      "learning_rate": 4.261585524908987e-05,
      "loss": 2.6321,
      "step": 565
    },
    {
      "epoch": 12.46,
      "grad_norm": 1.6536171436309814,
      "learning_rate": 4.249158351283414e-05,
      "loss": 2.5501,
      "step": 570
    },
    {
      "epoch": 12.57,
      "grad_norm": 2.082364320755005,
      "learning_rate": 4.2366459261474933e-05,
      "loss": 2.7045,
      "step": 575
    },
    {
      "epoch": 12.68,
      "grad_norm": 3.1343138217926025,
      "learning_rate": 4.224048859339175e-05,
      "loss": 2.6364,
      "step": 580
    },
    {
      "epoch": 12.79,
      "grad_norm": 2.99122953414917,
      "learning_rate": 4.211367764821722e-05,
      "loss": 2.6503,
      "step": 585
    },
    {
      "epoch": 12.9,
      "grad_norm": 2.024829864501953,
      "learning_rate": 4.198603260653792e-05,
      "loss": 2.6453,
      "step": 590
    },
    {
      "epoch": 13.01,
      "grad_norm": 2.1013944149017334,
      "learning_rate": 4.185755968959308e-05,
      "loss": 2.6756,
      "step": 595
    },
    {
      "epoch": 13.11,
      "grad_norm": 3.345149517059326,
      "learning_rate": 4.172826515897146e-05,
      "loss": 2.6072,
      "step": 600
    },
    {
      "epoch": 13.22,
      "grad_norm": 2.0921173095703125,
      "learning_rate": 4.1598155316306044e-05,
      "loss": 2.6775,
      "step": 605
    },
    {
      "epoch": 13.33,
      "grad_norm": 1.9780112504959106,
      "learning_rate": 4.146723650296701e-05,
      "loss": 2.6798,
      "step": 610
    },
    {
      "epoch": 13.44,
      "grad_norm": 2.3585338592529297,
      "learning_rate": 4.133551509975264e-05,
      "loss": 2.5418,
      "step": 615
    },
    {
      "epoch": 13.55,
      "grad_norm": 2.106301784515381,
      "learning_rate": 4.1202997526578276e-05,
      "loss": 2.6178,
      "step": 620
    },
    {
      "epoch": 13.66,
      "grad_norm": 2.444201707839966,
      "learning_rate": 4.1069690242163484e-05,
      "loss": 2.7604,
      "step": 625
    },
    {
      "epoch": 13.77,
      "grad_norm": 1.8051451444625854,
      "learning_rate": 4.093559974371725e-05,
      "loss": 2.7294,
      "step": 630
    },
    {
      "epoch": 13.88,
      "grad_norm": 2.9431896209716797,
      "learning_rate": 4.080073256662127e-05,
      "loss": 2.6389,
      "step": 635
    },
    {
      "epoch": 13.99,
      "grad_norm": 3.060418128967285,
      "learning_rate": 4.066509528411152e-05,
      "loss": 2.6634,
      "step": 640
    },
    {
      "epoch": 14.1,
      "grad_norm": 2.0075290203094482,
      "learning_rate": 4.052869450695776e-05,
      "loss": 2.6916,
      "step": 645
    },
    {
      "epoch": 14.21,
      "grad_norm": 2.7386865615844727,
      "learning_rate": 4.039153688314145e-05,
      "loss": 2.706,
      "step": 650
    },
    {
      "epoch": 14.32,
      "grad_norm": 2.882669687271118,
      "learning_rate": 4.02536290975317e-05,
      "loss": 2.5839,
      "step": 655
    },
    {
      "epoch": 14.43,
      "grad_norm": 2.7063851356506348,
      "learning_rate": 4.011497787155938e-05,
      "loss": 2.6345,
      "step": 660
    },
    {
      "epoch": 14.54,
      "grad_norm": 1.7728866338729858,
      "learning_rate": 3.997558996288965e-05,
      "loss": 2.6113,
      "step": 665
    },
    {
      "epoch": 14.64,
      "grad_norm": 2.0899062156677246,
      "learning_rate": 3.983547216509254e-05,
      "loss": 2.6386,
      "step": 670
    },
    {
      "epoch": 14.75,
      "grad_norm": 2.966677665710449,
      "learning_rate": 3.969463130731183e-05,
      "loss": 2.6404,
      "step": 675
    },
    {
      "epoch": 14.86,
      "grad_norm": 1.8099416494369507,
      "learning_rate": 3.955307425393224e-05,
      "loss": 2.6149,
      "step": 680
    },
    {
      "epoch": 14.97,
      "grad_norm": 1.9740931987762451,
      "learning_rate": 3.941080790424484e-05,
      "loss": 2.5981,
      "step": 685
    },
    {
      "epoch": 15.08,
      "grad_norm": 2.01714825630188,
      "learning_rate": 3.92678391921108e-05,
      "loss": 2.564,
      "step": 690
    },
    {
      "epoch": 15.19,
      "grad_norm": 2.4252054691314697,
      "learning_rate": 3.912417508562345e-05,
      "loss": 2.5909,
      "step": 695
    },
    {
      "epoch": 15.3,
      "grad_norm": 2.0233445167541504,
      "learning_rate": 3.897982258676867e-05,
      "loss": 2.5756,
      "step": 700
    },
    {
      "epoch": 15.41,
      "grad_norm": 1.9049184322357178,
      "learning_rate": 3.883478873108361e-05,
      "loss": 2.6182,
      "step": 705
    },
    {
      "epoch": 15.52,
      "grad_norm": 2.807706832885742,
      "learning_rate": 3.868908058731376e-05,
      "loss": 2.6562,
      "step": 710
    },
    {
      "epoch": 15.63,
      "grad_norm": 3.0991296768188477,
      "learning_rate": 3.85427052570685e-05,
      "loss": 2.6589,
      "step": 715
    },
    {
      "epoch": 15.74,
      "grad_norm": 3.1019067764282227,
      "learning_rate": 3.8395669874474915e-05,
      "loss": 2.5162,
      "step": 720
    },
    {
      "epoch": 15.85,
      "grad_norm": 1.8713304996490479,
      "learning_rate": 3.824798160583012e-05,
      "loss": 2.522,
      "step": 725
    },
    {
      "epoch": 15.96,
      "grad_norm": 2.5536465644836426,
      "learning_rate": 3.8099647649251986e-05,
      "loss": 2.6374,
      "step": 730
    },
    {
      "epoch": 16.07,
      "grad_norm": 2.2478325366973877,
      "learning_rate": 3.795067523432826e-05,
      "loss": 2.755,
      "step": 735
    },
    {
      "epoch": 16.17,
      "grad_norm": 1.648483395576477,
      "learning_rate": 3.780107162176429e-05,
      "loss": 2.568,
      "step": 740
    },
    {
      "epoch": 16.28,
      "grad_norm": 3.7869274616241455,
      "learning_rate": 3.765084410302909e-05,
      "loss": 2.555,
      "step": 745
    },
    {
      "epoch": 16.39,
      "grad_norm": 3.670029401779175,
      "learning_rate": 3.7500000000000003e-05,
      "loss": 2.6452,
      "step": 750
    },
    {
      "epoch": 16.5,
      "grad_norm": 2.783968925476074,
      "learning_rate": 3.7348546664605777e-05,
      "loss": 2.5808,
      "step": 755
    },
    {
      "epoch": 16.61,
      "grad_norm": 2.4255223274230957,
      "learning_rate": 3.719649147846832e-05,
      "loss": 2.5399,
      "step": 760
    },
    {
      "epoch": 16.72,
      "grad_norm": 3.265207052230835,
      "learning_rate": 3.704384185254288e-05,
      "loss": 2.6018,
      "step": 765
    },
    {
      "epoch": 16.83,
      "grad_norm": 2.012962818145752,
      "learning_rate": 3.689060522675689e-05,
      "loss": 2.5453,
      "step": 770
    },
    {
      "epoch": 16.94,
      "grad_norm": 2.975487232208252,
      "learning_rate": 3.673678906964727e-05,
      "loss": 2.6118,
      "step": 775
    },
    {
      "epoch": 17.05,
      "grad_norm": 2.437328338623047,
      "learning_rate": 3.6582400877996546e-05,
      "loss": 2.5509,
      "step": 780
    },
    {
      "epoch": 17.16,
      "grad_norm": 3.113149404525757,
      "learning_rate": 3.642744817646736e-05,
      "loss": 2.638,
      "step": 785
    },
    {
      "epoch": 17.27,
      "grad_norm": 3.129086971282959,
      "learning_rate": 3.627193851723577e-05,
      "loss": 2.5605,
      "step": 790
    },
    {
      "epoch": 17.38,
      "grad_norm": 3.720142126083374,
      "learning_rate": 3.611587947962319e-05,
      "loss": 2.558,
      "step": 795
    },
    {
      "epoch": 17.49,
      "grad_norm": 2.1802942752838135,
      "learning_rate": 3.5959278669726935e-05,
      "loss": 2.4937,
      "step": 800
    },
    {
      "epoch": 17.6,
      "grad_norm": 2.4899685382843018,
      "learning_rate": 3.580214372004956e-05,
      "loss": 2.5655,
      "step": 805
    },
    {
      "epoch": 17.7,
      "grad_norm": 2.733736991882324,
      "learning_rate": 3.564448228912682e-05,
      "loss": 2.5581,
      "step": 810
    },
    {
      "epoch": 17.81,
      "grad_norm": 2.6061012744903564,
      "learning_rate": 3.548630206115443e-05,
      "loss": 2.5646,
      "step": 815
    },
    {
      "epoch": 17.92,
      "grad_norm": 2.9398293495178223,
      "learning_rate": 3.532761074561355e-05,
      "loss": 2.64,
      "step": 820
    },
    {
      "epoch": 18.03,
      "grad_norm": 3.822050094604492,
      "learning_rate": 3.516841607689501e-05,
      "loss": 2.6143,
      "step": 825
    },
    {
      "epoch": 18.14,
      "grad_norm": 3.240427255630493,
      "learning_rate": 3.5008725813922386e-05,
      "loss": 2.5211,
      "step": 830
    },
    {
      "epoch": 18.25,
      "grad_norm": 3.0370168685913086,
      "learning_rate": 3.484854773977378e-05,
      "loss": 2.5152,
      "step": 835
    },
    {
      "epoch": 18.36,
      "grad_norm": 2.081047296524048,
      "learning_rate": 3.4687889661302576e-05,
      "loss": 2.5671,
      "step": 840
    },
    {
      "epoch": 18.47,
      "grad_norm": 4.242770195007324,
      "learning_rate": 3.452675940875686e-05,
      "loss": 2.5799,
      "step": 845
    },
    {
      "epoch": 18.58,
      "grad_norm": 5.160248279571533,
      "learning_rate": 3.436516483539781e-05,
      "loss": 2.5544,
      "step": 850
    },
    {
      "epoch": 18.69,
      "grad_norm": 2.2536041736602783,
      "learning_rate": 3.4203113817116957e-05,
      "loss": 2.4941,
      "step": 855
    },
    {
      "epoch": 18.8,
      "grad_norm": 3.1937031745910645,
      "learning_rate": 3.4040614252052305e-05,
      "loss": 2.5878,
      "step": 860
    },
    {
      "epoch": 18.91,
      "grad_norm": 2.335860252380371,
      "learning_rate": 3.387767406020343e-05,
      "loss": 2.6305,
      "step": 865
    },
    {
      "epoch": 19.02,
      "grad_norm": 2.7716612815856934,
      "learning_rate": 3.3714301183045385e-05,
      "loss": 2.5618,
      "step": 870
    },
    {
      "epoch": 19.13,
      "grad_norm": 2.643007755279541,
      "learning_rate": 3.355050358314172e-05,
      "loss": 2.4937,
      "step": 875
    },
    {
      "epoch": 19.23,
      "grad_norm": 2.4606404304504395,
      "learning_rate": 3.338628924375638e-05,
      "loss": 2.49,
      "step": 880
    },
    {
      "epoch": 19.34,
      "grad_norm": 2.234124183654785,
      "learning_rate": 3.322166616846458e-05,
      "loss": 2.4996,
      "step": 885
    },
    {
      "epoch": 19.45,
      "grad_norm": 3.4393913745880127,
      "learning_rate": 3.305664238076278e-05,
      "loss": 2.5396,
      "step": 890
    },
    {
      "epoch": 19.56,
      "grad_norm": 3.202657461166382,
      "learning_rate": 3.289122592367757e-05,
      "loss": 2.5711,
      "step": 895
    },
    {
      "epoch": 19.67,
      "grad_norm": 2.988165855407715,
      "learning_rate": 3.272542485937369e-05,
      "loss": 2.5432,
      "step": 900
    },
    {
      "epoch": 19.78,
      "grad_norm": 2.257720708847046,
      "learning_rate": 3.2559247268761115e-05,
      "loss": 2.5835,
      "step": 905
    },
    {
      "epoch": 19.89,
      "grad_norm": 2.4812440872192383,
      "learning_rate": 3.239270125110117e-05,
      "loss": 2.6011,
      "step": 910
    },
    {
      "epoch": 20.0,
      "grad_norm": 3.054295301437378,
      "learning_rate": 3.222579492361179e-05,
      "loss": 2.4526,
      "step": 915
    },
    {
      "epoch": 20.11,
      "grad_norm": 2.3782052993774414,
      "learning_rate": 3.205853642107192e-05,
      "loss": 2.4802,
      "step": 920
    },
    {
      "epoch": 20.22,
      "grad_norm": 2.7743964195251465,
      "learning_rate": 3.1890933895424976e-05,
      "loss": 2.4467,
      "step": 925
    },
    {
      "epoch": 20.33,
      "grad_norm": 2.193399429321289,
      "learning_rate": 3.172299551538164e-05,
      "loss": 2.4519,
      "step": 930
    },
    {
      "epoch": 20.44,
      "grad_norm": 2.3704917430877686,
      "learning_rate": 3.155472946602162e-05,
      "loss": 2.5026,
      "step": 935
    },
    {
      "epoch": 20.55,
      "grad_norm": 2.3345890045166016,
      "learning_rate": 3.138614394839476e-05,
      "loss": 2.5121,
      "step": 940
    },
    {
      "epoch": 20.66,
      "grad_norm": 3.6769206523895264,
      "learning_rate": 3.121724717912138e-05,
      "loss": 2.5099,
      "step": 945
    },
    {
      "epoch": 20.77,
      "grad_norm": 3.3187003135681152,
      "learning_rate": 3.104804738999169e-05,
      "loss": 2.5377,
      "step": 950
    },
    {
      "epoch": 20.87,
      "grad_norm": 2.8711345195770264,
      "learning_rate": 3.087855282756475e-05,
      "loss": 2.6043,
      "step": 955
    },
    {
      "epoch": 20.98,
      "grad_norm": 4.093137741088867,
      "learning_rate": 3.0708771752766394e-05,
      "loss": 2.5953,
      "step": 960
    },
    {
      "epoch": 21.09,
      "grad_norm": 3.2458410263061523,
      "learning_rate": 3.053871244048669e-05,
      "loss": 2.4354,
      "step": 965
    },
    {
      "epoch": 21.2,
      "grad_norm": 2.5772130489349365,
      "learning_rate": 3.0368383179176585e-05,
      "loss": 2.5405,
      "step": 970
    },
    {
      "epoch": 21.31,
      "grad_norm": 5.319322109222412,
      "learning_rate": 3.0197792270443982e-05,
      "loss": 2.4219,
      "step": 975
    },
    {
      "epoch": 21.42,
      "grad_norm": 2.3362972736358643,
      "learning_rate": 3.002694802864912e-05,
      "loss": 2.5601,
      "step": 980
    },
    {
      "epoch": 21.53,
      "grad_norm": 4.480556011199951,
      "learning_rate": 2.98558587804993e-05,
      "loss": 2.4892,
      "step": 985
    },
    {
      "epoch": 21.64,
      "grad_norm": 4.350553035736084,
      "learning_rate": 2.9684532864643122e-05,
      "loss": 2.4983,
      "step": 990
    },
    {
      "epoch": 21.75,
      "grad_norm": 2.573270797729492,
      "learning_rate": 2.9512978631264006e-05,
      "loss": 2.5522,
      "step": 995
    },
    {
      "epoch": 21.86,
      "grad_norm": 4.34506893157959,
      "learning_rate": 2.9341204441673266e-05,
      "loss": 2.5078,
      "step": 1000
    },
    {
      "epoch": 21.97,
      "grad_norm": 2.808000087738037,
      "learning_rate": 2.916921866790256e-05,
      "loss": 2.4761,
      "step": 1005
    },
    {
      "epoch": 22.08,
      "grad_norm": 2.68220591545105,
      "learning_rate": 2.8997029692295874e-05,
      "loss": 2.4597,
      "step": 1010
    },
    {
      "epoch": 22.19,
      "grad_norm": 4.24912166595459,
      "learning_rate": 2.8824645907100954e-05,
      "loss": 2.5049,
      "step": 1015
    },
    {
      "epoch": 22.3,
      "grad_norm": 3.9612669944763184,
      "learning_rate": 2.8652075714060295e-05,
      "loss": 2.5946,
      "step": 1020
    },
    {
      "epoch": 22.4,
      "grad_norm": 2.321424722671509,
      "learning_rate": 2.8479327524001636e-05,
      "loss": 2.442,
      "step": 1025
    },
    {
      "epoch": 22.51,
      "grad_norm": 4.727540493011475,
      "learning_rate": 2.8306409756428064e-05,
      "loss": 2.4459,
      "step": 1030
    },
    {
      "epoch": 22.62,
      "grad_norm": 3.8557896614074707,
      "learning_rate": 2.8133330839107608e-05,
      "loss": 2.4889,
      "step": 1035
    },
    {
      "epoch": 22.73,
      "grad_norm": 4.17302942276001,
      "learning_rate": 2.7960099207662532e-05,
      "loss": 2.5063,
      "step": 1040
    },
    {
      "epoch": 22.84,
      "grad_norm": 4.402253150939941,
      "learning_rate": 2.7786723305158136e-05,
      "loss": 2.4545,
      "step": 1045
    },
    {
      "epoch": 22.95,
      "grad_norm": 2.6420741081237793,
      "learning_rate": 2.761321158169134e-05,
      "loss": 2.4434,
      "step": 1050
    },
    {
      "epoch": 23.06,
      "grad_norm": 2.365680456161499,
      "learning_rate": 2.7439572493978736e-05,
      "loss": 2.4105,
      "step": 1055
    },
    {
      "epoch": 23.17,
      "grad_norm": 2.970679998397827,
      "learning_rate": 2.726581450494451e-05,
      "loss": 2.3983,
      "step": 1060
    },
    {
      "epoch": 23.28,
      "grad_norm": 2.354783773422241,
      "learning_rate": 2.7091946083307896e-05,
      "loss": 2.3918,
      "step": 1065
    },
    {
      "epoch": 23.39,
      "grad_norm": 3.150827646255493,
      "learning_rate": 2.6917975703170466e-05,
      "loss": 2.4884,
      "step": 1070
    },
    {
      "epoch": 23.5,
      "grad_norm": 2.7847702503204346,
      "learning_rate": 2.674391184360313e-05,
      "loss": 2.4334,
      "step": 1075
    },
    {
      "epoch": 23.61,
      "grad_norm": 3.753472089767456,
      "learning_rate": 2.656976298823284e-05,
      "loss": 2.4905,
      "step": 1080
    },
    {
      "epoch": 23.72,
      "grad_norm": 3.4611012935638428,
      "learning_rate": 2.6395537624829096e-05,
      "loss": 2.5581,
      "step": 1085
    },
    {
      "epoch": 23.83,
      "grad_norm": 2.9634251594543457,
      "learning_rate": 2.6221244244890336e-05,
      "loss": 2.4748,
      "step": 1090
    },
    {
      "epoch": 23.93,
      "grad_norm": 4.40963077545166,
      "learning_rate": 2.604689134322999e-05,
      "loss": 2.3981,
      "step": 1095
    },
    {
      "epoch": 24.04,
      "grad_norm": 2.21329665184021,
      "learning_rate": 2.587248741756253e-05,
      "loss": 2.4903,
      "step": 1100
    },
    {
      "epoch": 24.15,
      "grad_norm": 3.083101987838745,
      "learning_rate": 2.5698040968089225e-05,
      "loss": 2.4477,
      "step": 1105
    },
    {
      "epoch": 24.26,
      "grad_norm": 3.540137767791748,
      "learning_rate": 2.5523560497083926e-05,
      "loss": 2.5144,
      "step": 1110
    },
    {
      "epoch": 24.37,
      "grad_norm": 3.5205962657928467,
      "learning_rate": 2.5349054508478637e-05,
      "loss": 2.4638,
      "step": 1115
    },
    {
      "epoch": 24.48,
      "grad_norm": 3.33951735496521,
      "learning_rate": 2.517453150744904e-05,
      "loss": 2.412,
      "step": 1120
    },
    {
      "epoch": 24.59,
      "grad_norm": 3.7631826400756836,
      "learning_rate": 2.5e-05,
      "loss": 2.4557,
      "step": 1125
    },
    {
      "epoch": 24.7,
      "grad_norm": 2.6066832542419434,
      "learning_rate": 2.4825468492550964e-05,
      "loss": 2.4594,
      "step": 1130
    },
    {
      "epoch": 24.81,
      "grad_norm": 4.257205963134766,
      "learning_rate": 2.4650945491521372e-05,
      "loss": 2.4392,
      "step": 1135
    },
    {
      "epoch": 24.92,
      "grad_norm": 4.858757495880127,
      "learning_rate": 2.447643950291608e-05,
      "loss": 2.5169,
      "step": 1140
    },
    {
      "epoch": 25.03,
      "grad_norm": 4.070416450500488,
      "learning_rate": 2.4301959031910784e-05,
      "loss": 2.4997,
      "step": 1145
    },
    {
      "epoch": 25.14,
      "grad_norm": 4.7502336502075195,
      "learning_rate": 2.4127512582437485e-05,
      "loss": 2.4842,
      "step": 1150
    },
    {
      "epoch": 25.25,
      "grad_norm": 2.8164222240448,
      "learning_rate": 2.3953108656770016e-05,
      "loss": 2.3888,
      "step": 1155
    },
    {
      "epoch": 25.36,
      "grad_norm": 4.10616397857666,
      "learning_rate": 2.377875575510967e-05,
      "loss": 2.4117,
      "step": 1160
    },
    {
      "epoch": 25.46,
      "grad_norm": 3.24764084815979,
      "learning_rate": 2.3604462375170906e-05,
      "loss": 2.4483,
      "step": 1165
    },
    {
      "epoch": 25.57,
      "grad_norm": 3.2320103645324707,
      "learning_rate": 2.3430237011767167e-05,
      "loss": 2.4902,
      "step": 1170
    },
    {
      "epoch": 25.68,
      "grad_norm": 4.029989242553711,
      "learning_rate": 2.3256088156396868e-05,
      "loss": 2.4593,
      "step": 1175
    },
    {
      "epoch": 25.79,
      "grad_norm": 4.837875843048096,
      "learning_rate": 2.3082024296829536e-05,
      "loss": 2.5853,
      "step": 1180
    },
    {
      "epoch": 25.9,
      "grad_norm": 3.5902416706085205,
      "learning_rate": 2.2908053916692117e-05,
      "loss": 2.426,
      "step": 1185
    },
    {
      "epoch": 26.01,
      "grad_norm": 1.9076547622680664,
      "learning_rate": 2.2734185495055503e-05,
      "loss": 2.4524,
      "step": 1190
    },
    {
      "epoch": 26.12,
      "grad_norm": 2.7655446529388428,
      "learning_rate": 2.2560427506021266e-05,
      "loss": 2.3717,
      "step": 1195
    },
    {
      "epoch": 26.23,
      "grad_norm": 2.7638556957244873,
      "learning_rate": 2.238678841830867e-05,
      "loss": 2.4414,
      "step": 1200
    },
    {
      "epoch": 26.34,
      "grad_norm": 3.575471878051758,
      "learning_rate": 2.2213276694841866e-05,
      "loss": 2.4113,
      "step": 1205
    },
    {
      "epoch": 26.45,
      "grad_norm": 2.7083661556243896,
      "learning_rate": 2.2039900792337474e-05,
      "loss": 2.3875,
      "step": 1210
    },
    {
      "epoch": 26.56,
      "grad_norm": 4.073393821716309,
      "learning_rate": 2.186666916089239e-05,
      "loss": 2.5095,
      "step": 1215
    },
    {
      "epoch": 26.67,
      "grad_norm": 3.0759992599487305,
      "learning_rate": 2.1693590243571938e-05,
      "loss": 2.4516,
      "step": 1220
    },
    {
      "epoch": 26.78,
      "grad_norm": 4.809901237487793,
      "learning_rate": 2.1520672475998373e-05,
      "loss": 2.5,
      "step": 1225
    },
    {
      "epoch": 26.89,
      "grad_norm": 2.9302878379821777,
      "learning_rate": 2.1347924285939714e-05,
      "loss": 2.401,
      "step": 1230
    },
    {
      "epoch": 26.99,
      "grad_norm": 3.3039402961730957,
      "learning_rate": 2.117535409289905e-05,
      "loss": 2.4428,
      "step": 1235
    },
    {
      "epoch": 27.1,
      "grad_norm": 2.443258762359619,
      "learning_rate": 2.1002970307704132e-05,
      "loss": 2.3867,
      "step": 1240
    },
    {
      "epoch": 27.21,
      "grad_norm": 2.488567590713501,
      "learning_rate": 2.0830781332097446e-05,
      "loss": 2.4336,
      "step": 1245
    },
    {
      "epoch": 27.32,
      "grad_norm": 3.4901556968688965,
      "learning_rate": 2.0658795558326743e-05,
      "loss": 2.4223,
      "step": 1250
    },
    {
      "epoch": 27.43,
      "grad_norm": 3.315229892730713,
      "learning_rate": 2.0487021368736003e-05,
      "loss": 2.3541,
      "step": 1255
    },
    {
      "epoch": 27.54,
      "grad_norm": 2.951272964477539,
      "learning_rate": 2.031546713535688e-05,
      "loss": 2.4989,
      "step": 1260
    },
    {
      "epoch": 27.65,
      "grad_norm": 3.7014811038970947,
      "learning_rate": 2.0144141219500705e-05,
      "loss": 2.4731,
      "step": 1265
    },
    {
      "epoch": 27.76,
      "grad_norm": 3.1045243740081787,
      "learning_rate": 1.9973051971350888e-05,
      "loss": 2.4772,
      "step": 1270
    },
    {
      "epoch": 27.87,
      "grad_norm": 2.7251968383789062,
      "learning_rate": 1.980220772955602e-05,
      "loss": 2.5114,
      "step": 1275
    },
    {
      "epoch": 27.98,
      "grad_norm": 4.8662238121032715,
      "learning_rate": 1.963161682082342e-05,
      "loss": 2.3229,
      "step": 1280
    },
    {
      "epoch": 28.09,
      "grad_norm": 5.2037553787231445,
      "learning_rate": 1.946128755951332e-05,
      "loss": 2.349,
      "step": 1285
    },
    {
      "epoch": 28.2,
      "grad_norm": 4.288341045379639,
      "learning_rate": 1.9291228247233605e-05,
      "loss": 2.446,
      "step": 1290
    },
    {
      "epoch": 28.31,
      "grad_norm": 5.210068702697754,
      "learning_rate": 1.912144717243525e-05,
      "loss": 2.4152,
      "step": 1295
    },
    {
      "epoch": 28.42,
      "grad_norm": 2.5746872425079346,
      "learning_rate": 1.895195261000831e-05,
      "loss": 2.3549,
      "step": 1300
    },
    {
      "epoch": 28.52,
      "grad_norm": 4.216084957122803,
      "learning_rate": 1.8782752820878634e-05,
      "loss": 2.4741,
      "step": 1305
    },
    {
      "epoch": 28.63,
      "grad_norm": 5.37358283996582,
      "learning_rate": 1.8613856051605243e-05,
      "loss": 2.4001,
      "step": 1310
    },
    {
      "epoch": 28.74,
      "grad_norm": 3.6173505783081055,
      "learning_rate": 1.8445270533978388e-05,
      "loss": 2.4252,
      "step": 1315
    },
    {
      "epoch": 28.85,
      "grad_norm": 3.507467746734619,
      "learning_rate": 1.827700448461836e-05,
      "loss": 2.4224,
      "step": 1320
    },
    {
      "epoch": 28.96,
      "grad_norm": 4.072767734527588,
      "learning_rate": 1.8109066104575023e-05,
      "loss": 2.4595,
      "step": 1325
    },
    {
      "epoch": 29.07,
      "grad_norm": 4.735693454742432,
      "learning_rate": 1.7941463578928086e-05,
      "loss": 2.4019,
      "step": 1330
    },
    {
      "epoch": 29.18,
      "grad_norm": 2.8641066551208496,
      "learning_rate": 1.7774205076388206e-05,
      "loss": 2.325,
      "step": 1335
    },
    {
      "epoch": 29.29,
      "grad_norm": 3.6184470653533936,
      "learning_rate": 1.7607298748898842e-05,
      "loss": 2.4426,
      "step": 1340
    },
    {
      "epoch": 29.4,
      "grad_norm": 3.2939038276672363,
      "learning_rate": 1.744075273123889e-05,
      "loss": 2.3898,
      "step": 1345
    },
    {
      "epoch": 29.51,
      "grad_norm": 3.504326581954956,
      "learning_rate": 1.7274575140626318e-05,
      "loss": 2.3321,
      "step": 1350
    },
    {
      "epoch": 29.62,
      "grad_norm": 3.4017646312713623,
      "learning_rate": 1.7108774076322443e-05,
      "loss": 2.335,
      "step": 1355
    },
    {
      "epoch": 29.73,
      "grad_norm": 3.3392391204833984,
      "learning_rate": 1.6943357619237226e-05,
      "loss": 2.3949,
      "step": 1360
    },
    {
      "epoch": 29.84,
      "grad_norm": 2.952181339263916,
      "learning_rate": 1.677833383153542e-05,
      "loss": 2.5048,
      "step": 1365
    },
    {
      "epoch": 29.95,
      "grad_norm": 3.246154308319092,
      "learning_rate": 1.6613710756243626e-05,
      "loss": 2.3806,
      "step": 1370
    },
    {
      "epoch": 30.05,
      "grad_norm": 3.2340292930603027,
      "learning_rate": 1.6449496416858284e-05,
      "loss": 2.4398,
      "step": 1375
    },
    {
      "epoch": 30.16,
      "grad_norm": 4.7577643394470215,
      "learning_rate": 1.6285698816954624e-05,
      "loss": 2.3756,
      "step": 1380
    },
    {
      "epoch": 30.27,
      "grad_norm": 3.909966230392456,
      "learning_rate": 1.612232593979658e-05,
      "loss": 2.3604,
      "step": 1385
    },
    {
      "epoch": 30.38,
      "grad_norm": 4.4209136962890625,
      "learning_rate": 1.5959385747947698e-05,
      "loss": 2.4217,
      "step": 1390
    },
    {
      "epoch": 30.49,
      "grad_norm": 4.993971347808838,
      "learning_rate": 1.5796886182883053e-05,
      "loss": 2.4545,
      "step": 1395
    },
    {
      "epoch": 30.6,
      "grad_norm": 3.4732532501220703,
      "learning_rate": 1.56348351646022e-05,
      "loss": 2.3751,
      "step": 1400
    },
    {
      "epoch": 30.71,
      "grad_norm": 3.5623276233673096,
      "learning_rate": 1.547324059124315e-05,
      "loss": 2.3719,
      "step": 1405
    },
    {
      "epoch": 30.82,
      "grad_norm": 3.681424379348755,
      "learning_rate": 1.5312110338697426e-05,
      "loss": 2.3778,
      "step": 1410
    },
    {
      "epoch": 30.93,
      "grad_norm": 3.6803464889526367,
      "learning_rate": 1.5151452260226224e-05,
      "loss": 2.4402,
      "step": 1415
    },
    {
      "epoch": 31.04,
      "grad_norm": 4.329118251800537,
      "learning_rate": 1.4991274186077632e-05,
      "loss": 2.4458,
      "step": 1420
    },
    {
      "epoch": 31.15,
      "grad_norm": 2.180776596069336,
      "learning_rate": 1.4831583923104999e-05,
      "loss": 2.33,
      "step": 1425
    },
    {
      "epoch": 31.26,
      "grad_norm": 4.534682750701904,
      "learning_rate": 1.467238925438646e-05,
      "loss": 2.3423,
      "step": 1430
    },
    {
      "epoch": 31.37,
      "grad_norm": 4.2668070793151855,
      "learning_rate": 1.4513697938845572e-05,
      "loss": 2.4161,
      "step": 1435
    },
    {
      "epoch": 31.48,
      "grad_norm": 4.218076229095459,
      "learning_rate": 1.4355517710873184e-05,
      "loss": 2.3425,
      "step": 1440
    },
    {
      "epoch": 31.58,
      "grad_norm": 4.449320316314697,
      "learning_rate": 1.4197856279950438e-05,
      "loss": 2.4908,
      "step": 1445
    },
    {
      "epoch": 31.69,
      "grad_norm": 4.373895168304443,
      "learning_rate": 1.4040721330273062e-05,
      "loss": 2.3873,
      "step": 1450
    },
    {
      "epoch": 31.8,
      "grad_norm": 4.118370056152344,
      "learning_rate": 1.388412052037682e-05,
      "loss": 2.3552,
      "step": 1455
    },
    {
      "epoch": 31.91,
      "grad_norm": 4.808157444000244,
      "learning_rate": 1.3728061482764238e-05,
      "loss": 2.3388,
      "step": 1460
    },
    {
      "epoch": 32.02,
      "grad_norm": 3.018151044845581,
      "learning_rate": 1.3572551823532654e-05,
      "loss": 2.4209,
      "step": 1465
    },
    {
      "epoch": 32.13,
      "grad_norm": 4.922707557678223,
      "learning_rate": 1.3417599122003464e-05,
      "loss": 2.4288,
      "step": 1470
    },
    {
      "epoch": 32.24,
      "grad_norm": 3.211679220199585,
      "learning_rate": 1.3263210930352737e-05,
      "loss": 2.3288,
      "step": 1475
    },
    {
      "epoch": 32.35,
      "grad_norm": 3.102727174758911,
      "learning_rate": 1.3109394773243117e-05,
      "loss": 2.3863,
      "step": 1480
    },
    {
      "epoch": 32.46,
      "grad_norm": 4.10581111907959,
      "learning_rate": 1.2956158147457115e-05,
      "loss": 2.3387,
      "step": 1485
    },
    {
      "epoch": 32.57,
      "grad_norm": 3.439565658569336,
      "learning_rate": 1.280350852153168e-05,
      "loss": 2.398,
      "step": 1490
    },
    {
      "epoch": 32.68,
      "grad_norm": 2.9784770011901855,
      "learning_rate": 1.2651453335394231e-05,
      "loss": 2.4037,
      "step": 1495
    },
    {
      "epoch": 32.79,
      "grad_norm": 4.462327003479004,
      "learning_rate": 1.2500000000000006e-05,
      "loss": 2.412,
      "step": 1500
    }
  ],
  "logging_steps": 5,
  "max_steps": 2250,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 50,
  "save_steps": 100,
  "total_flos": 6.890116721594204e+17,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
